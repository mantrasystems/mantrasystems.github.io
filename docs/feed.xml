<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://www.mantrasystems.co.uk/feed.xml" rel="self" type="application/atom+xml" /><link href="https://www.mantrasystems.co.uk/" rel="alternate" type="text/html" /><updated>2025-05-12T10:16:24+01:00</updated><id>https://www.mantrasystems.co.uk/feed.xml</id><title type="html">Mantra Systems</title><entry><title type="html">SOTA Exclusions: The Ongoing Quest for Clarity</title><link href="https://www.mantrasystems.co.uk/articles/sota-exclusions-ongoing-quest-for-clarity" rel="alternate" type="text/html" title="SOTA Exclusions: The Ongoing Quest for Clarity" /><published>2025-05-12T00:00:00+01:00</published><updated>2025-05-09T11:11:28+01:00</updated><id>https://www.mantrasystems.co.uk/articles/sota-exclusions-ongoing-quest-for-clarity</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/sota-exclusions-ongoing-quest-for-clarity"><![CDATA[<p>Anyone who’s worked on a <a href="/medical-device-regulatory-consulting-services/mdr-consulting/clinical-evaluation">Clinical Evaluation</a> knows the drill by now: you’re deep into the narrative, you’ve built a clear clinical picture, the literature is supporting your claims, and then you hit the <a href="/articles/literature-search-sota-review-and-clinical-evaluation">State of the Art (SOTA) section</a>.</p>
      <p>It’s one of the most deceptively complex parts of the Clinical Evaluation. The SOTA section, at its core, is about establishing the clinical context of your device. You’re identifying technologies, treatments, and medical devices that are comparable or similar to yours and demonstrating how your device measures up against them. In theory, this sounds straightforward, but here’s where things get complicated. While you’re focused on proving your device stands up to clinical scrutiny, regulators are equally interested in what you’re not including, and more importantly, <strong>why</strong>.</p>
      <h2 id="the-real-challenge-what-you-leave-out-matters-just-as-much">The Real Challenge: What You Leave Out Matters Just As Much</h2>
      <p>SOTA exclusions aren’t just a box-ticking exercise, they’re a true test of your reasoning and your ability to defend your choices with confidence. If you exclude devices, alternative technologies, or treatment options that seem relevant without a solid justification in place, the regulators will notice. It’s not enough to say, “This device isn’t comparable”, you need to explain why. Is it the indication? The mechanism of action? The patient population? Is there insufficient clinical evidence? Every exclusion must be logical, traceable, and transparent.</p>
      <h2 id="why-this-keeps-coming-up">Why This Keeps Coming Up</h2>
      <p>This isn’t just a one-off issue, as it’s seen across many submissions. Most teams feel confident about what they’ve included in their SOTA, but the rationale for exclusions often ends up vague, underdeveloped, or missing entirely. The truth is, SOTA requires interpretation, and that’s exactly why it’s so frequently dissected. If a seemingly relevant comparator or treatment is left out without a strong explanation, it stands out.</p>
      <p>Regulators don’t just want to see what you included, they want to understand why you didn’t include something else. Without that clarity, they may assume the exclusion was unintentional or that your review wasn’t comprehensive. In short, it keeps coming up because exclusions are easy to challenge and hard to justify unless you’ve done the thinking and documented it clearly.</p>
      <figure class="intrinsic">
        <img src="/assets/images/content/articles/inline/sota-quest.png" alt="How to Overcome the SOTA Exclusion Trap process diagram." loading="lazy" width="400" height="428" />
        <figcaption>How to Overcome the SOTA Exclusion Trap</figcaption>
      </figure>
      <h3 id="1-start-with-a-broad-landscape-then-narrow-it-down">1. Start with a broad landscape, then narrow it down</h3>
      <p>Don’t begin with what you want to exclude; start with everything that could be relevant. List out all comparable technologies, treatments, and devices, even those you know won’t make the final cut. From there, document your rationale for keeping or excluding each one.</p>
      <p><em>Tip: <a href="https://www.medical-device-regulation.eu/wp-content/uploads/2019/05/2_7_1_rev4_en.pdf">MedDev 2.7/1 rev 4</a> requires the use of carefully formulated inclusion / exclusion criteria, augmented by an objective, non-biased, systematic screening and review process such as PICO (patient characteristics, type of intervention, control, and outcome queries), or PRISMA (The Preferred Reporting Items for Systematic Reviews and Meta-Analyses).</em></p>
      <h3 id="2-be-specific-in-your-exclusions">2. Be specific in your exclusions</h3>
      <p>Vague justifications are one of the fastest ways to undermine an otherwise strong Clinical Evaluation. A sentence like “This technology was excluded due to differences in application” is almost guaranteed to prompt questions. Instead, give clear, defensible reasons tied to your device’s design, function, and clinical use. Examples of strong justifications:</p>
      <ul>
        <li>Article reporting outcomes or clinical scenarios entirely unrelated to the subject matter.</li>
        <li>Not related to the intended purpose of the subject device. For example, an article focused on wound closure techniques would be an inappropriate focus when the subject matter of a review is the safety or performance of a hip implant.</li>
        <li>The literature contains unsubstantiated opinions</li>
      </ul>
      <p><em>Tip: If you’re unsure how specific to be, err on the side of more. Specificity signals competence and confidence.</em></p>
      <h3 id="3-link-exclusions-back-to-the-devices-intended-purpose">3. Link exclusions back to the device’s intended purpose</h3>
      <p>The <a href="/articles/a-medical-devices-intended-purpose-what-is-the-point">intended purpose isn’t just regulatory terminology</a>, it’s your compass. Every inclusion or exclusion should align with it. If something is excluded because it falls outside your indication, say so. If your device is single-use and another is reusable, that difference might be clinically meaningful.</p>
      <p><em>Tip: Write your intended purpose in plain language and keep it visible during writing and editing. Use it as a live reference point when making decisions on what to include or exclude.</em></p>
      <h3 id="4-document-the-process-and-justification-not-just-the-outcome">4. Document the process and justification (not just the outcome)</h3>
      <p>A polished summary is important, but so is the trail you took to get there. Regulators don’t just want to see the final result; they want to know you followed a systematic process to get there.</p>
      <p>It may be helpful to use exclusion codes to codify reasons for exclusions (which may include, for example, “unrelated topic” or “article not available in English,” etc).</p>
      <p><em>Tip: Save your search strings, selection criteria, and full extractions of literature from your databases. Include a short methods section describing how you determine exclusions. This small step can prevent a lot of back-and-forth during review.</em></p>
      <h3 id="5-avoid-over-exclusion">5. Avoid over-exclusion</h3>
      <p>While it’s important to justify exclusions, it’s equally important not to exclude too many devices or treatments, as an overly narrow SOTA can look suspicious to regulators. Try to keep the scope broad, including all potentially relevant comparators, and only exclude those with a well-supported rationale.</p>
      <p><em>Tip: Aim for a balance. Make sure your exclusions are justified, but ensure that your SOTA still includes a reasonable number of relevant sources, where deemed appropriate. It demonstrates thoroughness and strengthens your argument for your device’s clinical positioning.</em></p>
      <h2 id="final-thought-clarity-and-transparency-will-always-be-your-strongest-defence">Final Thought: Clarity and Transparency Will Always Be Your Strongest Defence</h2>
      <p>SOTA exclusions will likely remain one of the more complex aspects of a Clinical Evaluation submission. However, with a well-structured approach, <a href="/eu-mdr-compliance/literature-search-protocol-sota-review">the right tools</a>, and a strong focus on clinical reasoning, you can turn this challenge into an opportunity. Clarity in the rationale for exclusions can enhance the strength of your submission by demonstrating thoroughness, transparency, and attention to detail.</p>
      <p>By articulating your exclusions in a manner that’s easy to follow and backed by solid evidence, you make it easier for regulators to see your reasoning, leading to smoother reviews and fewer questions along the way. Navigating exclusions doesn’t have to be daunting, but it does require precision, thorough documentation, and a clear understanding of both clinical and regulatory expectations.</p>
      <p>If you’re feeling uncertain about your SOTA exclusions or finding it difficult to manage the comparator landscape, <a href="/contact">don’t hesitate to reach out to Mantra Systems</a>. We offer tailored support to guide you through the process, ensuring your Clinical Evaluation is both comprehensive and compliant. Let us help you get it right the first time.</p>
      ]]></content><author><name>kamiya-crabtree</name></author><category term="Clinical Evaluation" /><summary type="html"><![CDATA[SOTA exclusions aren’t just a box-ticking exercise, they’re a true test of your reasoning and your ability to defend your choices with confidence.]]></summary></entry><entry><title type="html">Clinical Evaluation Masterclass: It is not clear that any systematic search methods were used for the literature review – Episode 3</title><link href="https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-3-not-clear-that-any-systematic-search-methods-were-used-for-the-literature-review" rel="alternate" type="text/html" title="Clinical Evaluation Masterclass: It is not clear that any systematic search methods were used for the literature review – Episode 3" /><published>2025-05-08T00:00:00+01:00</published><updated>2025-05-07T15:31:50+01:00</updated><id>https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-3-not-clear-that-any-systematic-search-methods-were-used-for-the-literature-review</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-3-not-clear-that-any-systematic-search-methods-were-used-for-the-literature-review"><![CDATA[<p>Clinical evaluation is a cornerstone of compliance under the EU Medical Device Regulation (MDR), directly tied to the safety and performance of a medical device throughout its lifecycle.</p>
    <p>Yet, despite its importance, non-conformities in clinical evaluation reports remain one of the most common reasons for delays, rejections, and costly remediation work during conformity assessment.</p>
    <p>When a clinical evaluation falls short - whether due to vague safety and performance objectives, inadequate literature appraisal, or poorly documented search methods - manufacturers face serious consequences. These may include prolonged Notified Body reviews, increased regulatory scrutiny, and even market withdrawal of devices.</p>
    <p>Addressing clinical evaluation non-conformities isn’t just about avoiding negative outcomes; it’s about building a robust, evidence-based foundation that demonstrates your device meets essential safety and performance requirements. For manufacturers, mastering this process means faster approvals, fewer surprises during conformity assessments, and ultimately, safer outcomes for patients.</p>
    <p>This video series breaks down the most common clinical evaluation non-conformities, explains <em>why</em> they keep happening, and most importantly, shows you how to <strong><em>fix (or avoid) them for</em></strong> good.</p>
    <h2 id="episode-3-it-is-not-clear-that-systematic-search-methods-have-been-used-for-the-literature-review">Episode 3: “It is not clear that systematic search methods have been used for the literature review”</h2>
    <p>Whether you’re new to clinical evaluation or refining an existing process, this video will help you design a robust and transparent search strategy that aligns with MDR expectations.</p>
    <p>In this episode, we’ll break down:</p>
    <ul>
      <li>What a <strong>systematic search method</strong> is</li>
      <li>Key components of a systematic search</li>
      <li>How systematic search methodology should be <strong>applied during Clinical Evaluation</strong></li>
    </ul>
    <p>By the end of the episode, you’ll be able to confidently design and implement a systematic literature search for both State-of-the-art (SOTA) devices and the Device Under Evaluation (DUE) when conducting Clinical Evaluations.</p>
    <p>At <em>Mantra Systems</em>, we specialise in delivering high-quality, MDR-compliant Clinical Evaluation Reports that stand up to Notified Body scrutiny. Whether you need a full CER, a literature review strategy, or expert input on addressing non-conformities, our team is here to help. <a href="/contact">Contact us today for a free, no obligation consultation</a> on how we can support your clinical evaluation needs.</p>
    <p>Missed previous episodes? Catch up here:</p>
    <ul>
      <li><a href="/articles/clinical-evaluation-masterclass-1-overcoming-non-conformities">Episode 1: “Safety and Performance objectives lack specific and measurable acceptance criteria”</a></li>
      <li><a href="/articles/clinical-evaluation-masterclass-2-appraisal-of-literature-sources-has-not-been-conducted-properly">Episode 2: “Appraisal of literature sources has not been conducted properly”</a></li>
    </ul>
    <hr />
    <div class="small-text">
      <strong>Transcript follows:</strong>
      <p>Okay. Hi everyone and welcome to episode three in our ongoing series on how to fix clinical evaluation nonconformities. It’s Paul here from Mantra Systems. It’s my pleasure to take you through this latest installment of the series and uh let’s get to it.</p>
      <p>So um as a reminder, this series is focused on addressing a key problem in um achieving success following submission of a clinical evaluation and that’s that nonconformities are actually very common under the MDR. But although it’s necessary to ensure that nonconformities are resolved, um they can be quite difficult to interpret and it may not always be clear how to fully address any nonconformities which arise, which in turn can lead to further rounds of review with increased costs, loss, time, stress, and worry.</p>
      <p>And this series is intended to help you avoid common reasons for nonconformities. So, episode by episode, we’re working through common non-conformities one at a time and digging deep into what they mean and importantly, how to solve or avoid them in the first place. And where possible we tried to link to general principles for optimal conduct of clinical evaluation.</p>
      <hr />
      <p><strong>The Nonconformity in Focus</strong></p>
      <p>So this is episode three and this time we’re going to look at the following nonconformity:</p>
      <blockquote>
        <p>It is not clear that systematic search methods have been used for the literature review.</p>
      </blockquote>
      <p>So in order to unpack this one, we need to consider:</p>
      <ol>
        <li>What is a systematic search method?</li>
        <li>What are the components of a systematic search?</li>
        <li>How should systematic search methodology be applied during clinical evaluation?</li>
      </ol>
      <p>Understanding those areas will help avoid this non-conformity arising.</p>
      <hr />
      <p><strong>What Is a Systematic Search Method?</strong></p>
      <p>Well, it is a structured and planned approach to finding relevant sources for literature reviews. And in this case we’re talking about its application to clinical evaluation.</p>
      <p>From a clinical evaluation perspective, it’s relevant in two sectors:</p>
      <ul>
        <li>The literature search that’s conducted for the state-of-the-art.</li>
        <li>The search that needs to be done for the device under evaluation.</li>
      </ul>
      <p>Ideally, they are two separate searches, documented and recorded separately.</p>
      <hr />
      <p><strong>Why It Matters</strong></p>
      <p>A systematic search method applied correctly helps ensure several things:</p>
      <ul>
        <li>Literature searches are conducted according to a validated prespecified plan. In other words, you’re not making it up as you go along.</li>
        <li>Literature searches have considered all relevant factors to ensure full coverage of clinical use of either state-of-the-art devices or the device under evaluation.</li>
        <li>Searches are reproducible so they can be conducted more than once and result in the same outcome.</li>
        <li>Bias and the risk of accidentally excluding relevant sources are minimized.</li>
      </ul>
      <hr />
      <p><strong>The Three Key Stages</strong></p>
      <p>On a very basic level, there are three key stages to a systematic literature search:</p>
      <ol>
        <li><strong>Intended purpose</strong></li>
        <li><strong>Research questions</strong></li>
        <li><strong>Search terms</strong></li>
      </ol>
      <hr />
      <p><strong>Intended Purpose</strong></p>
      <p>The intended purpose places the literature search into context and helps form an input to the definition of research questions.</p>
      <p>Spending time on the intended purpose helps map out:</p>
      <ul>
        <li>The types of patients and conditions that fall under the scope of the search,</li>
        <li>The context of clinical use, and so on.</li>
      </ul>
      <p>It is a vital input to the scope of the literature search and also shows traceability to how the device is used and applied in clinical practice. So it is an important albeit rather basic stage from the perspective of a literature search. But don’t skip it.</p>
      <hr />
      <p><strong>Research Questions</strong></p>
      <p>The next stage is to begin crafting research questions.</p>
      <p>There’s lots of ways to do this, but ultimately a research question or a set of research questions just reduces what you want to achieve from the search to a list of simple questions. And it’s generally preferred that a validated format is used. PICO is one of them. We’re going to take a look at PICO in a moment.</p>
      <p>It’s a good stage to explore and refine the scope of the search.</p>
      <blockquote>
        <p>It’s very easy to change research questions—much easier than changing search terms.</p>
      </blockquote>
      <p>Because with search terms, you have to rerun a search each time you change it, and that can be quite labor-intensive, especially if you’ve already started with inclusion/exclusion screening.</p>
      <p>So research questions are worth spending time on because they form a vital input to the specific searches that you’ll run.</p>
      <hr />
      <p><strong>The PICO Framework</strong></p>
      <p>Let’s have a quick look at one validated method—it’s not the only one—but PICO is an important one.</p>
      <p><strong>PICO</strong> stands for:</p>
      <ul>
        <li><strong>P</strong>atient</li>
        <li><strong>I</strong>ntervention</li>
        <li><strong>C</strong>omparator</li>
        <li><strong>O</strong>utcome</li>
      </ul>
      <p>It’s just a way of breaking down the search or the scope of search into these specific stages.</p>
      <p>Example:</p>
      <ul>
        <li><strong>Patients</strong>: Patients with osteoarthritis of the hip who are candidates for total hip replacement surgery.</li>
        <li><strong>Intervention</strong>: Ceramic-on-ceramic hip implants.</li>
        <li><strong>Comparators</strong>: Metal-on-metal or metal-on-polyester hip implants.</li>
        <li><strong>Outcomes</strong>: Pain VAS and range of motion.</li>
      </ul>
      <p>That’s just one way of framing research questions using the PICO system.</p>
      <hr />
      <p><strong>Stage Three: Search Terms</strong></p>
      <p>What we’ve got here is a series of screen grabs just doing an example search on PubMed.</p>
      <p>Example breakdown:</p>
      <ul>
        <li>Search 18: Osteoarthritis (produces many results)</li>
        <li>Search 19: Total hip replacement or hip arthroplasty</li>
        <li>Search 20: THR (abbreviation of total hip replacement)</li>
      </ul>
      <blockquote>
        <p>You want to avoid repeating the same term over and over again, especially when using boolean AND, because it will look for multiple instances unnecessarily.</p>
      </blockquote>
      <p>In PubMed’s advanced search:</p>
      <ul>
        <li>You can combine individual searches using boolean operators.</li>
        <li>Click the three dots next to a search and select ‘Add query’.</li>
        <li>Combine subsequent searches using <strong>AND</strong>.</li>
      </ul>
      <p>Result:</p>
      <ul>
        <li>A refined composite search that significantly narrows down the results (e.g., to 51 highly relevant studies).</li>
      </ul>
      <hr />
      <p><strong>Additional Systematic Search Considerations</strong></p>
      <p><strong>1. Database Specification</strong></p>
      <p>State in advance which databases will be used in your search protocol.</p>
      <p>Examples:</p>
      <ul>
        <li>PubMed</li>
        <li>Google Scholar</li>
        <li>Embase</li>
        <li>Cochrane</li>
        <li>WDI</li>
      </ul>
      <blockquote>
        <p>Usually, using a minimum of two is best practice. Otherwise, it may suggest bias.</p>
      </blockquote>
      <hr />
      <p><strong>2. Inclusion/Exclusion Screening</strong></p>
      <p>Refine your results to only those that are relevant.</p>
      <ul>
        <li>Define exclusion criteria in advance and document them in the search protocol.</li>
        <li>Prevents designing criteria reactively to include only favorable results.</li>
      </ul>
      <p>Example:</p>
      <ul>
        <li>Code E1: Exclude in vitro, pre-clinical, or animal studies because their results may not transfer to humans.</li>
      </ul>
      <p>This ensures every excluded result has a justification.</p>
      <hr />
      <p><strong>3. Handling Duplicate Results</strong></p>
      <p>Problems:</p>
      <ul>
        <li>Same study found more than once.</li>
        <li>Same dataset appearing in multiple publications.</li>
      </ul>
      <p>Risks increase when including <strong>systematic reviews</strong>, which:</p>
      <ul>
        <li>May duplicate primary data sources.</li>
        <li>Can create overlap and confusion.</li>
      </ul>
      <p>Tactics:</p>
      <ul>
        <li>Look out for recurring author names, shared recruitment techniques, and subsidiary publications.</li>
        <li>Consider excluding systematic reviews and focus on primary sources.</li>
      </ul>
      <blockquote>
        <p>Including duplicates risks skewing your analysis by over-weighting repeated data.</p>
      </blockquote>
      <hr />
      <p><strong>4. Use of Filters</strong></p>
      <p>Search databases offer filters, but:</p>
      <ul>
        <li>Use with caution.</li>
        <li>Carefully justify their use.</li>
        <li>Filters vary across databases.</li>
      </ul>
      <p>Common justifiable filter: <strong>time limits</strong>, especially for <strong>state-of-the-art</strong> searches.</p>
      <blockquote>
        <p>For fast-evolving fields like software as a medical device, old studies may no longer be relevant.</p>
      </blockquote>
      <hr />
      <p><strong>5. Reproducibility</strong></p>
      <p>Another person should be able to:</p>
      <ul>
        <li>Rerun the search,</li>
        <li>Achieve the same results.</li>
      </ul>
      <hr />
      <p><strong>Application to Clinical Evaluation</strong></p>
      <ul>
        <li>Create a detailed <strong>search protocol</strong> for both the <strong>SOT</strong> and <strong>device-specific</strong> reviews.</li>
        <li>Document:
          <ul>
            <li>Research questions</li>
            <li>Search terms</li>
            <li>Exclusion criteria and justifications</li>
          </ul>
        </li>
        <li>Justify use of filters.</li>
        <li>Remove duplicate results.</li>
        <li>Use validated methods.</li>
        <li>Ensure full <strong>reproducibility</strong>.</li>
      </ul>
      <hr />
      <p>In summary, for <strong>Nonconformity Number Three</strong>, we’ve covered:</p>
      <ul>
        <li>What a systematic search method is,</li>
        <li>The components of a systematic search,</li>
        <li>How systematic search methodology should be applied during clinical evaluation.</li>
      </ul>
      <hr />
      <p>As a reminder, Mantra Systems is a clinical evaluation specialist.</p>
      <ul>
        <li>Extensive experience across the market,</li>
        <li>Clinical evaluations for all classes of medical device,</li>
        <li>A current 100% submission success record.</li>
      </ul>
      <p>Feel free to reach out to us if you have any questions about clinical evaluation or you’d like to speak with any of our specialists with regard to your own.</p>
      <p>That concludes episode three of this non-conformity series.</p>
      <p>Many thank yous for your kind attention.</p>
    </div>
    ]]></content><author><name>paul-hercock</name></author><category term="Clinical Evaluation" /><category term="MDR" /><summary type="html"><![CDATA[Addressing non-conformities isn’t just about avoiding negative outcomes; it’s about building a robust, evidence-based foundation.]]></summary></entry><entry><title type="html">Regulatory Writing Deadlines: The Pressure to Get It Right the First Time</title><link href="https://www.mantrasystems.co.uk/articles/regulatory-writing-deadlines-the-pressure-to-get-it-right-the-first-time" rel="alternate" type="text/html" title="Regulatory Writing Deadlines: The Pressure to Get It Right the First Time" /><published>2025-04-30T00:00:00+01:00</published><updated>2025-04-29T19:07:17+01:00</updated><id>https://www.mantrasystems.co.uk/articles/regulatory-writing-deadlines-the-pressure-to-get-it-right-the-first-time</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/regulatory-writing-deadlines-the-pressure-to-get-it-right-the-first-time"><![CDATA[<p>Anyone who’s worked in the medical device industry knows that regulatory deadlines aren’t just part of the process—they define it.</p>
  <p>Whether you’re drafting a <a href="/medical-device-regulatory-consulting-services/mdr-consulting/clinical-evaluation">Clinical Evaluation Report</a> (CER) or preparing an <a href="medical-device-regulatory-consulting-services/ivdr-consulting">IVDR</a> or <a href="/medical-device-regulatory-consulting-services/mdr-consulting/">MDR submission</a>, there’s one golden rule everyone learns quickly: you feel the pressure to get it right the first time, because in this field, second chances are rare—and costly.</p>
  <p>And that pressure? It doesn’t just come from the clock. It comes from the complexity of the content, the coordination required across teams, and the ever-present reality that regulators aren’t grading on a curve.</p>
  <h2 id="why-the-clock-is-always-ticking-and-coordinating-moving-pieces">Why the Clock Is Always Ticking and Coordinating Moving Pieces</h2>
  <p>In regulatory writing, time is always a pressure point. While the device may be ready, the trial complete, and the data in hand, without well-aligned and structured documentation, the product can’t move forward. Writing typically happens late in the development cycle, and the responsibility to wrap it up quickly falls squarely on the regulatory team.</p>
  <p>Revisions might be allowed, but delays can have significant consequences, pushing back launch dates, sales forecasts, and patient access. In a competitive market, even a few weeks’ delay can make a huge impact.</p>
  <p>In other words, the sooner it’s done, the sooner it’s in the hands of the people who need it.</p>
  <p>Take the Clinical Evaluation Report (CER), for example. It requires input from clinical, risk, and quality teams, each working on different timelines. If one team is delayed or key data is missing, the regulatory writer is left to fill in the gaps. Yet the final document still needs to be clear, compliant, and scientifically sound. Managing these moving parts under intense time pressure is where the real challenge lies.</p>
  <p>And despite all this, the report must be coherent. It must make sense to regulators, and it must do so in language that’s both technically precise and easy to follow. Balancing scientific depth with regulatory clarity, while coordinating teams and racing deadlines, is what makes regulatory writing uniquely demanding.</p>
  <h2 id="time-pressure--technical-complexity--a-tough-combo">Time Pressure + Technical Complexity = A Tough Combo</h2>
  <p>Regulatory writing is anything but straightforward. It’s high-pressure work that requires interpreting complex data, aligning with evolving guidelines, and managing competing timelines. You’re expected to translate clinical, safety, or performance data into documents that meet both internal and regulatory expectations.</p>
  <p>Timelines are rarely in your favour, and last-minute data, shifting guidance, and delayed inputs from other teams are common. Yet, the final document still has to be accurate and on time.</p>
  <p>Under pressure, shortcuts can be tempting: citing claims without solid evidence, glossing over exclusions, or relying on unchecked references. These might seem small, but regulators spot them instantly. One inconsistency can spark a chain of questions, delaying approval. The internal cost involves pulling teams back into work they thought was finished, creating stress and draining resources.</p>
  <h2 id="how-to-manage-the-pressure">How To Manage the Pressure</h2>
  <p>While the pressure may never disappear entirely, you can develop strategies to manage it effectively and deliver strong submissions consistently. Here’s how:</p>
  <h3 id="1-build-time-buffers-wherever-possible-and-stick-to-them">1. Build Time Buffers Wherever Possible (and Stick to Them)</h3>
  <p>Time buffers aren’t just a luxury, they’re a necessity as things will go wrong. A key staff member will be on leave, a literature search might surface late-breaking studies, or a formatting requirement will change mid-review. Planning for the unexpected is what keeps your timeline intact.</p>
  <p>How to do it:</p>
  <ul>
    <li>Create internal deadlines at least 3–5 working days before submission.</li>
    <li>Schedule a formal buffer period at the end of the review cycle for last-minute fixes, formatting, or unanticipated input.</li>
    <li>Use a Gantt chart or timeline visual to map dependencies and highlight bottlenecks early.</li>
  </ul>
  <p><em>Tip: Treat internal deadlines like external ones. Lock them in calendars, send reminders, and keep them visible in team trackers so they’re taken just as seriously.</em></p>
  <h3 id="2-use-templates-and-checklists-to-reduce-cognitive-load">2. Use Templates and Checklists to Reduce Cognitive Load</h3>
  <p>Regulatory writing isn’t just about words, it’s about structure, traceability, and completeness. Using <a href="/enable-ce-mark/technical-document-templates/">well-maintained templates</a>, <a href="/eu-mdr-compliance">guides</a>, and <a href="/enable-ce-mark/technical-document-templates/mdr-compliance-checker-gap-analysis-tool">submission checklists</a> allows writers to focus on content rather than reformatting or double-checking document layout.</p>
  <p>Best practices:</p>
  <ul>
    <li>Maintain validated templates for CERs, PMS Plans, SSCPs, and more.</li>
    <li>Create a library of pre-approved justifications, definitions, and citations.</li>
    <li>Use trackers or collaborative SharePoint folders to keep everything in one place.</li>
  </ul>
  <p><em>Tip: Maintain a document library with pre-approved phrasing for recurring justifications, definitions, and regulatory language to speed up writing and ensure consistency.</em></p>
  <h3 id="3-clarify-the-why-behind-every-section">3. Clarify the “Why” Behind Every Section</h3>
  <p>When working fast, it’s easy to fall into the trap of filling sections with text that sounds right but lacks substance. Writers who stay close to the underlying intent of each section, whether it’s explaining benefit-risk, post-market follow-up, or equivalence, are better positioned to write persuasively and pass regulatory scrutiny.</p>
  <p><em>Tip: Before writing each section, note down the question you’re answering. For example: “What is the clinical benefit of this device, and how is it supported by evidence?” Then answer that clearly and directly.</em></p>
  <h3 id="4-align-stakeholders-early">4. Align Stakeholders Early</h3>
  <p>Regulatory writing relies on input from clinical, risk, and quality teams. When these inputs arrive late or out of sync, the whole writing timeline suffers.</p>
  <p>How to improve:</p>
  <ul>
    <li>Hold a project kick-off call to align timelines and expectations across teams.</li>
    <li>Share a high-level plan with key stakeholders early, especially if input will be staggered.</li>
    <li>Confirm who’s reviewing what, and when, to avoid overlaps or gaps.</li>
  </ul>
  <p>Example: For an MDR CER, a short cross-functional alignment meeting early in the project avoided weeks of back-and-forth during the review phase.</p>
  <h3 id="5-break-big-tasks-into-smaller-wins">5. Break Big Tasks into Smaller Wins</h3>
  <p>Large documents like CERs or SSCPs, and especially full Technical Files can feel overwhelming. Breaking the work into logical sections and setting mini-deadlines helps manage workload and keep momentum.</p>
  <p>What works:</p>
  <ul>
    <li>Set internal milestones for key sections (e.g. clinical data, Annex II documents, risk-benefit).</li>
    <li>Track progress visibly so the team stays motivated and aligned.</li>
    <li>Divide the document by section (e.g. clinical background, state of the art, risk-benefit analysis, PMS summary) and assign clear ownership where possible.</li>
  </ul>
  <p><em>Tip: Celebrate section sign-offs as mini-successes—they build confidence and maintain pace. Also, create a visual dashboard (even a basic traffic-light tracker in Excel) to show section status, e.g., red for in progress, amber for under review, green for approved. It helps everyone see at a glance where things stand and what needs focus next.</em></p>
  <p><strong>Bonus benefit:</strong> Smaller chunks are also easier to quality check, making it more likely that errors or inconsistencies are spotted early—before they snowball into regulatory queries.</p>
  <h3 id="6-learn-from-every-submission">6. Learn from Every Submission</h3>
  <p>Every regulatory submission, whether smooth, delayed, or somewhere in between, is packed with lessons. The key is capturing those insights while they’re still fresh and feeding them back into future projects. That’s how teams evolve from “just getting it done” to building consistently efficient, high-quality submissions.</p>
  <p>How to do it:</p>
  <ul>
    <li>Hold post-submission meetings: Within a week or two of submitting, schedule a short debrief session. Ask: What went well? What slowed us down? What could we do differently next time?</li>
    <li>Involve everyone: Writers, project managers, and any contributors should have a voice. Often, the people managing literature, quality checks, or stakeholder feedback have the best insights into where time was lost, or saved.</li>
    <li>Document and apply learnings: Keep a running log of tips, regulatory feedback, time-saving methods, and submission pitfalls.</li>
  </ul>
  <h2 id="conclusion-high-pressure-high-standardsmanageable-with-the-right-tools">Conclusion: High Pressure, High Standards—Manageable with the Right Tools</h2>
  <p>There’s no denying that writing regulatory documents under tight deadlines can be intense. The expectations are high, the work is technical, and the timelines are often compressed. But by planning strategically, working collaboratively, and using the right tools and frameworks, you can consistently produce documents that meet both internal expectations and regulatory requirements.</p>
  <p>The key is not to chase perfection, but to build processes that support quality from the very start. Because getting it right the first time might not always be possible, but it should always be the goal.</p>
  <p>When everything comes together, when the data aligns, the document flows, and the submission is accepted without question, there’s a distinct sense of pride that accompanies the moment.</p>
  <p>Whether you’re preparing a technical file, responding to questions from a notified body, or working toward approval, our team provides practical, tailored support at every stage. We offer strategic guidance across regulatory frameworks, assist with technical documentation, and help you stay aligned with evolving requirements.</p>
  <p>If you’re looking for a trusted partner to take the pressure out of compliance, <a href="/contact">get in touch with Mantra Systems today</a>.</p>
  ]]></content><author><name>kamiya-crabtree</name></author><category term="Clinical Evaluation" /><category term="MDR" /><category term="SaMD" /><summary type="html"><![CDATA[Anyone who’s worked in the medical device industry knows that regulatory deadlines aren’t just part of the process—they define it.]]></summary></entry><entry><title type="html">IEC 62366-1:2015 Demystified – Essential Usability Testing for Medical Devices</title><link href="https://www.mantrasystems.co.uk/articles/essential-usability-testing-for-medical-devices-iec-62366-1-2015-demystified" rel="alternate" type="text/html" title="IEC 62366-1:2015 Demystified – Essential Usability Testing for Medical Devices" /><published>2025-04-23T00:00:00+01:00</published><updated>2025-04-23T14:28:13+01:00</updated><id>https://www.mantrasystems.co.uk/articles/essential-usability-testing-for-medical-devices-iec-62366-1-2015-demystified</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/essential-usability-testing-for-medical-devices-iec-62366-1-2015-demystified"><![CDATA[<p>Creating a usability file is a crucial part of the product development process for medical devices, including <a href="/solutions/regulatory-consulting-for-samd-software-as-a-medical-device">Software as a Medical Device (SaMD)</a>, to ensure user-centred design and safety. IEC 62366-1 provides a structured approach to ensure that devices are designed with the user in mind and that potential risks associated with their use are identified and mitigated.</p>
  <h2 id="what-is-iec-62366-a-crucial-standard-for-medical-device-usability">What is IEC 62366? A Crucial Standard for Medical Device Usability</h2>
  <p><a href="https://www.iso.org/standard/63179.html">According to ISO</a>, ‘<q>IEC 62366 specifies a process for a manufacturer to analyse, specify, develop and evaluate the useability of a medical device as it relates to safety. The first edition of IEC 62366-1, together with the first edition of IEC 62366-2, cancels and replaces the first edition of IEC 62366 published in 2007 and its Amendment 1 (2014)</q>’</p>
  <ol>
    <li>IEC 62366-1 — Application of usability engineering to medical devices</li>
    <li>IEC 62366-2 — Guidance on the application of usability to engineering to medical devices</li>
  </ol>
  <p>This guideline provides medical device manufacturers with a structured approach to integrate human factors, supporting them in assessing, defining, creating, and testing the usability of their medical devices.</p>
  <h2 id="navigating-the-process-key-steps-in-writing-your-usability-file">Navigating the Process: Key Steps in Writing Your Usability File</h2>
  <p>What exactly should be included in a Usability Engineering File? How can you ensure that all the requirements outlined in the standard are met? What steps do you need to take to ensure compliance? These are common questions that many people face when beginning to create their usability files. By understanding the essential components and requirements, you’ll be well-equipped to develop a comprehensive Usability Engineering File that meets all necessary standards and helps ensure your medical device is both safe and effective for use.</p>
  <figure class="intrinsic">
    <img src="/assets/images/content/articles/inline/define-the-scope-of-your-usability-file.png" alt="A diagram showing the IEC 62366 process." loading="lazy" width="400" height="670" />
  </figure>
  <h3 id="step-1-define-the-scope">Step 1: Define the Scope</h3>
  <p>This encompasses the specific user group, the intended purpose, and the context in which the device will be utilised.</p>
  <p><strong>Intended Users:</strong> Who will be using the device? This could include healthcare professionals, patients, or other relevant parties.</p>
  <p><strong>Intended Use:</strong> What is the device designed to accomplish? This refers to the specific tasks the device is intended for, such as monitoring, diagnosing, or therapeutic applications.</p>
  <p><strong>Use Environment:</strong> Where will the device be used? The setting can significantly influence the device’s usability, whether it’s intended for use in a home environment or a clinical/hospital setting.</p>
  <h3 id="step-2-conducting-a-risk-analysis">Step 2: Conducting a Risk Analysis</h3>
  <p>IEC 62366-1 mandates that you perform a comprehensive risk analysis to identify potential use-related hazards—situations where user errors may lead to harm or malfunction of the device.</p>
  <p><strong>Hazard Identification:</strong> Identify potential hazards that may result from user errors or design flaws. For example, in a medical device like a ventilator, if the user interface is poorly designed, a healthcare professional could mistakenly adjust the settings, leading to inappropriate ventilation rates for the patient. This could result in either hypoventilation (insufficient oxygen delivery) or hyperventilation (excessive oxygen delivery), both of which could cause severe harm to the patient.</p>
  <p><strong>Risk Assessment:</strong> Assess the likelihood and severity of harm associated with each identified hazard. This involves evaluating whether a specific user action could result in patient harm, an incorrect diagnosis, or delayed treatment, ensuring that risks are appropriately managed and mitigated.</p>
  <h3 id="step-3-define-user-interface-ui-design-requirements">Step 3: Define User Interface (UI) Design Requirements</h3>
  <p>Following the analysis of user needs and risks, the next crucial step is to establish UI design requirements tailored to the target user group. Chapter 5.2. requires you to identify, as part of the risk analysis, all parts and characteristics of the User Interface that are related to the safety of the medical device.</p>
  <p><strong>User Capabilities:</strong> Consider the cognitive, physical, and sensory abilities of the users. For instance, if the device is intended for elderly patients, ensure the interface is intuitive, with large text and simplified navigation.</p>
  <p><strong>Design Guidelines:</strong> These should focus on clear labelling, well-organised controls, alarms, and instructions for use (IFU). Specific adaptations may be necessary for certain groups, such as non-professional users or individuals with visual impairments.</p>
  <p><strong>Task Optimisation:</strong> Structure the UI to reduce the likelihood of user error. Important tasks should be straightforward to access and perform, eliminating unnecessary steps that could lead to mistakes.</p>
  <h3 id="step-4-formative-evaluations">Step 4: Formative Evaluations</h3>
  <p>The formative evaluation section involves the testing of early prototypes or even design concepts with users in order to identify any potential problems prior to the final design being confirmed and completed. Once risks are identified, this process will repeat until these are mitigated.</p>
  <p><strong>User Testing:</strong> Carry out usability testing with representative users to assess the device’s interface and interaction design. This could involve techniques such as cognitive walkthroughs, task analysis, and heuristic evaluations to identify potential issues and areas for improvement.</p>
  <p>Were critical use errors found? Are further product improvements necessary? If you have answered “yes” to either of these questions at this point, go back as far as necessary in the Usability Engineering process and iterate through it from this point on until no new and critical problems occur during use. If this is the case, you are ready for the last big step: performing the summative evaluation.</p>
  <h3 id="step-5-summative-evaluation">Step 5: Summative Evaluation</h3>
  <p>The goal is to prove that the medical device (and its Interface) can be used without unacceptable residual risks for users, patients, and third parties. The hazard-related use scenarios previously defined are tested.</p>
  <p>The summative evaluation focuses on validating the final design to ensure that the device meets the defined usability goals and operates safely and effectively in real-world conditions.</p>
  <p><strong>Final Usability Testing:</strong> Conduct usability testing on the final version of the device to confirm that it functions as expected and that users can interact with it safely. This typically involves more realistic use scenarios, which may take place in clinical or home environments.</p>
  <p><strong>Task Performance Metrics:</strong> Evaluate whether users can complete key tasks successfully, such as using the device without errors, interpreting information accurately, and following safety instructions correctly. These metrics help ensure the device’s usability and overall safety in practical use.</p>
  <h3 id="step-6-post-market-surveillance-pms-and-usability-monitoring">Step 6: Post-Market Surveillance (PMS) and Usability Monitoring</h3>
  <p>Once the product has been released to the market, ongoing monitoring of its usability is crucial. Post-market surveillance helps to identify any emerging issues or areas where improvements can be made.</p>
  <p><strong>Incident Reporting:</strong> Set up a system to collect feedback and reports related to user issues. This should include complaints, adverse events, or any usability failures encountered by users.</p>
  <p><strong>Usability Improvements:</strong> If usability issues are identified after the product has been launched, they should be addressed through iterative improvements or design updates to enhance performance and user experience.</p>
  <h3 id="step-7-compiling-the-usability-file">Step 7: Compiling the Usability File</h3>
  <p>The Usability Engineering File should provide a thorough and organised record of the entire usability process, from initial risk analysis and design specifications to testing and post-market surveillance. It is essential that the file demonstrates full compliance with the usability requirements of IEC 62366-1, showing that every step has been taken to guarantee the device’s safety and effectiveness.</p>
  <p>The file should include:</p>
  <ul>
    <li>The device’s scope and intended use</li>
    <li>A detailed hazard analysis and risk assessment</li>
    <li>Clear design specifications and user interface requirements</li>
    <li>Results from formative and summative testing</li>
    <li>A post-market plan for ongoing usability monitoring</li>
  </ul>
  <h2 id="your-path-to-iec-62366-compliance-key-takeaways">Your Path to IEC 62366 Compliance: Key Takeaways</h2>
  <p>The Usability Engineering process, as meticulously outlined in IEC 62366-1, plays a crucial role in ensuring the safety and effectiveness of medical devices. By closely integrating with Risk Management, this process helps manufacturers identify and address potential use-related hazards that could jeopardise user safety, patient health, or device performance. Effective implementation, alongside comprehensive documentation, is not only vital for regulatory compliance but also for minimising the risks associated with human error. Ultimately, the goal is to create devices that are intuitive, safe, and reliable and that can stand up to real-world challenges. Following the steps of the Usability Engineering process ensures that these objectives are met and that safety is prioritised in every phase of development.</p>
  <p>Standards like IEC 62366-1:2015 are just one part of the increasingly complex regulatory landscape facing medical device companies. At Mantra Systems, we specialise in helping you navigate these challenges with clarity and confidence.</p>
  <p>Whether you’re preparing a technical file, responding to questions from a notified body, or working toward approval, our team provides practical, tailored support at every stage. We offer strategic guidance across regulatory frameworks, assist with technical documentation, and help you stay aligned with evolving requirements.</p>
  <p>If you’re looking for a trusted partner to take the pressure out of compliance, <a href="/contact">get in touch with Mantra Systems today</a>.</p>
  ]]></content><author><name>kamiya-crabtree</name></author><category term="MDR" /><category term="SaMD" /><summary type="html"><![CDATA[What should be included in a Usability Engineering File? What steps do you need to take to ensure compliance and meet standards?]]></summary></entry><entry><title type="html">Unlocking the Secrets of Effective IVD Device Performance Evaluation</title><link href="https://www.mantrasystems.co.uk/articles/unlocking-the-secrets-of-effective-ivd-device-performance-evaluation" rel="alternate" type="text/html" title="Unlocking the Secrets of Effective IVD Device Performance Evaluation" /><published>2025-04-16T00:00:00+01:00</published><updated>2025-04-23T14:28:13+01:00</updated><id>https://www.mantrasystems.co.uk/articles/unlocking-the-secrets-of-effective-ivd-device-performance-evaluation</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/unlocking-the-secrets-of-effective-ivd-device-performance-evaluation"><![CDATA[<p>Performance evaluation is a critical requirement for in vitro diagnostic (IVD) devices and forms part of the technical documentation necessary for placing an IVD device on the EU market. The requirements for performance evaluation are described within Chapter VI of the Regulation on <em>in vitro</em> diagnostic medical devices <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32017R0746#anx_XIII">(EU 2017/746) (IVDR)</a> and supported by Annexes I, II, III and XIII.</p>
  <h2 id="understanding-performance-evaluation-what-you-need-to-know">Understanding Performance Evaluation: What You Need to Know</h2>
  <p>Chapter VI of the IVDR states:</p>
  <blockquote>
    <p>“<em>A performance evaluation shall follow a defined and methodologically sound procedure for the demonstration of the following, in accordance with this Article and with Part A of Annex XIII</em>:</p>
    <ul>
      <li><em>scientific validity;</em></li>
      <li><em>analytical performance;</em></li>
      <li><em>clinical performance;</em></li>
    </ul>
  </blockquote>
  <figure>
    <img src="/assets/images/content/articles/inline/ivd-performance-evaluation-1.png" alt="" loading="lazy" width="442" height="322" />
  </figure>
  <p>The clinical evidence derived from the <a href="/medical-device-regulatory-consulting-services/ivdr-consulting">performance evaluation</a> shall provide scientifically valid assurance that the relevant general safety and performance requirements (GSPRs) set out in Annex I are fulfilled under normal conditions of use.</p>
  <h2 id="a-closer-look-at-article-56">A Closer Look at Article 56</h2>
  <p>Article 56 of the IVDR refers to the GSPRs set out in Annex I, specifically relating to the performance characteristics described in Chapter I and Section 9 of Annex I. It also addresses the acceptability of the benefit-risk ratio, as outlined in Sections 1 and 8 of Annex I, which is based on the scientific validity, analytical performance, and clinical performance of the IVD. Annex XIII specifies how IVD manufacturers must plan, conduct, and document the performance evaluation.</p>
  <p>Manufacturers are required to establish a standard operating procedure (SOP) for IVD performance evaluations, ensuring that it covers the entire device lifecycle. The IVDR defines performance evaluation as a continuous process that closely interfaces with risk management, as referenced in Annex VII, Section 4.5.4, and Annex XIII, Section 1.1.</p>
  <h2 id="mastering-the-performance-evaluation-plan">Mastering the Performance Evaluation Plan</h2>
  <p>A device-specific Performance Evaluation Plan (PEP) must be prepared in accordance with Annex XIII, Section 1.1 of the IVDR. The PEP should outline the current state of the art in relation to the medical, diagnostic, and technology-related fields relevant to the device. It must also reference applicable standards, common specifications, guidelines, or best practice documents.</p>
  <p>The PEP should define the intended purpose and characteristics of the device, including the analyte or biomarker it detects, its intended use, target user, and indications for use. Additionally, it must outline how the device demonstrates compliance with the GSPRs through its performance evaluation. The plan should also briefly describe the methods used to assess the device’s analytical and clinical performance, benefit-risk considerations, and the Post-Market Performance Follow-up (PMPF) plan.</p>
  <h2 id="scientific-validity-explained">Scientific Validity Explained</h2>
  <p>Scientific validity refers to the association between the analyte measured by the device and the clinical condition or physiological state. This association is documented in a Scientific Validity Report (SVR). The SVR should establish the device as state-of-the-art and demonstrate its clinical benefit. It should provide evidence of the clinical background, the clinical condition, the significance of the analyte, currently-used diagnostic options, similar and/or equivalent devices, and recommendations from professional societies to support the IVD’s performance and intended use.</p>
  <figure>
    <img src="/assets/images/content/articles/inline/ivd-performance-evaluation-2.png" alt="" loading="lazy" width="442" height="322" />
  </figure>
  <h2 id="navigating-analytical-performance">Navigating Analytical Performance</h2>
  <p>Section 9.1 of Annex I outlines the analytical performance requirements. However, as it is not possible to anticipate all scenarios regarding device types and intended purposes, the responsibility lies with the manufacturer to provide a rationale for any characteristics that are not applicable to their device. <a href="https://www.iso.org/standard/79868.html">EN ISO 18113-3:2022</a>, <em>In vitro diagnostic medical devices — Information supplied by the manufacturer (labelling) — Part 3: In vitro diagnostic instruments for professional use</em>, offers guidance on performance characteristics that should be included in the labelling for instruments. Analytical performance must always be demonstrated through analytical performance studies, either using existing data or generating new evidence.</p>
  <figure>
    <img src="/assets/images/content/articles/inline/ivd-performance-evaluation-3.png" alt="" loading="lazy" width="442" height="322" />
  </figure>
  <h2 id="clinical-performance-uncovered">Clinical Performance Uncovered</h2>
  <p>Clinical performance is the “<em>ability of a device to yield results which are correlated with a defined clinical/physiological/pathological condition or state following the target population and intended user</em>”. Clinical performance provides evidence of how effectively the IVD device delivers results, diagnoses, or guides treatment decisions. Annex XIII, Section 1.2.3 of the IVDR outlines multiple options for gathering clinical performance data to substantiate the clinical evidence of the device.</p>
  <p>For devices measuring analytes that are associated with a clinical condition that have medical decision points, clinical performance data is required and should form part of the clinical performance report (CPR). Typical clinical performance data could be diagnostic sensitivity and specificity, area under the curve, negative predictive value, and positive predictive value.</p>
  <p>For devices measuring analytes without clear medical decision points or for devices measuring analytes that are not (yet) associated with a clinical condition, clinical performance may be defined as correlation with a physiological state, or a justification for omission of clinical performance studies may be considered. Typical data presented in the CPR would be negative percent agreement and positive percent agreement.</p>
  <p>Demonstration of the clinical performance of a device is based on one or a combination of the following sources:</p>
  <figure>
    <img src="/assets/images/content/articles/inline/ivd-performance-evaluation-4.png" alt="" loading="lazy" width="442" height="322" />
  </figure>
  <h2 id="the-key-steps-in-crafting-a-performance-evaluation-report">The Key Steps in Crafting a Performance Evaluation Report</h2>
  <p>In the Performance Evaluation Report (PER), you will need to summarise all the results related to Scientific Validity, Analytical Performance, and Clinical Performance to demonstrate conformity with the relevant GSPRs for your device, as referred to in Annex I of the IVDR. In the PER, you should evaluate the clinical evidence in light of the current state of the art in medicine, technology, the intended purpose, and the device’s positive benefit-risk ratio.</p>
  <p> Annex VIII, Part A (1.3.2) of the IVDR outlines the specific components of the PER and specifies that it must include:</p>
  <ul>
    <li>the justification for the approach taken to gather the clinical evidence;</li>
    <li>the literature search methodology and the literature search protocol and literature search report of a literature review;</li>
    <li>the technology on which the device is based, the intended purpose of the device, and any claims made about the device’s performance or safety;</li>
    <li>the nature and extent of the scientific validity and the analytical and clinical performance data that has been evaluated;</li>
    <li>the clinical evidence as the acceptable performances against the state of the art in medicine;</li>
    <li>any new conclusions derived from PMPF reports</li>
  </ul>
  <p>PERs for Class C and D devices must be updated at least annually, whereas PERs for Class A and B devices should be updated as needed, although at least a three-year review cycle would be recommended. As this document is multi-disciplinary, it is crucial that the manufacturer organises the writing and maintenance of this document and fully integrates it into the infrastructure of the quality management system (QMS).</p>
  <h2 id="ensuring-ongoing-success-with-post-market-performance-follow-up">Ensuring Ongoing Success with Post-Market Performance Follow-Up</h2>
  <p>Post-Market Performance Follow-Up (PMPF) is defined in Part B of Annex XIII as ‘a continuous process that updates the performance evaluation’ and goes on to state ‘<em>with the aim of confirming the safety, performance and scientific validity throughout the expected lifetime of the device, of ensuring the continued acceptability of the benefit-risk ratio and of detecting emerging risks on the basis of factual evidence</em>’.</p>
  <p>PMPF essentially serves to continue supporting the PEP/PER throughout the device’s lifecycle and is particularly useful for monitoring scientific or clinical developments that may impact the device’s performance and safety.</p>
  <p>Outputs from PMPF are written in the PMPF evaluation report, where it is then determined if those outputs result in:</p>
  <ul>
    <li>An update to the associated performance evaluation documentation and a revision of the risk management reports</li>
    <li>A CAPA being raised</li>
    <li>Further PMPF studies being conducted</li>
  </ul>
  <h2 id="ensuring-ivd-success-key-takeaways-for-performance-evaluation">Ensuring IVD Success: Key Takeaways for Performance Evaluation</h2>
  <p>Conducting a performance evaluation for IVDs is a critical step in ensuring their safety, reliability, and effectiveness. By following a structured process—starting with understanding regulatory requirements, defining intended use, and conducting rigorous analytical and clinical testing—you can demonstrate that your IVD meets the necessary standards for market approval. Continuous post-market surveillance further ensures that the device continues to deliver value and contributes to better patient outcomes.</p>
  <p>At Mantra Systems, we simplify the process, ensuring your device meets all regulatory requirements for EU market access. Let us take the complexity out of performance evaluation so you can focus on delivering safe, effective solutions. <a href="/contact">Book a call today</a>.</p>
  ]]></content><author><name>kamiya-crabtree</name></author><category term="IVDR" /><summary type="html"><![CDATA[Whether preparing for a new EU market submission or strengthening existing documentation, this guide is your overview of performance evaluation under IVDR.]]></summary></entry><entry><title type="html">Clinical Evaluation Masterclass: Appraisal of literature sources has not been conducted properly - Episode 2</title><link href="https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-2-appraisal-of-literature-sources-has-not-been-conducted-properly" rel="alternate" type="text/html" title="Clinical Evaluation Masterclass: Appraisal of literature sources has not been conducted properly - Episode 2" /><published>2025-04-09T00:00:00+01:00</published><updated>2025-04-09T12:47:28+01:00</updated><id>https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-2-appraisal-of-literature-sources-has-not-been-conducted-properly</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-2-appraisal-of-literature-sources-has-not-been-conducted-properly"><![CDATA[<p>A non-conformity in clinical evaluation arises when a <strong>Clinical Evaluation Plan (CEP) or Report (CER)</strong> fails to meet regulatory standards or Notified Body (NB) expectations.</p>
  <p>These deficiencies such as incomplete data, unclear safety criteria, or inadequate evidence analysis indicate that the device’s safety and performance are not sufficiently demonstrated, requiring correction before approval.</p>
  <p>For manufacturers, such non-conformities can result in <strong>delayed approvals, extended review cycles, and costly rework</strong>, as unresolved issues block NB acceptance. Proactively addressing non-conformities helps streamline audits, accelerate certifications, and strengthen regulatory compliance, ensuring smoother market access.</p>
  <p>This video series breaks down the most common clinical evaluation non-conformities, explains <em>why</em> they keep happening, and most importantly, shows you how to <em><strong>fix (or avoid) them</strong></em> for good.</p>
  <h2 id="episode-2-appraisal-of-literature-sources-has-not-been-conducted-properly">Episode 2: “Appraisal of literature sources has not been conducted properly”</h2>
  <p>One of the most frequent reasons for CER rejection is poor appraisal of literature sources. In this episode, we cover:</p>
  <ul>
    <li>What <em>‘appraisal’</em> really means and the purpose it serves</li>
    <li>Key characteristics of an appropriately conducted appraisal process</li>
    <li>How to practically implement appraisals during clinical evaluation and why it matters</li>
  </ul>
  <p>A well-documented appraisal strategy is essential for demonstrating safety and performance through clinical data. Ensuring the appraisal is systematic, objective, and traceable can significantly reduce regulatory pushback.</p>
  <p>If you missed it, <a href="/articles/clinical-evaluation-masterclass-1-overcoming-non-conformities">be sure to check out Episode 1:”Safety and Performance Objectives Lack Specific and measurable acceptance criteria”</a> where we explore how to define specific, measurable objectives and develop practical acceptance criteria that meet Notified Body expectations.</p>
  <p>Stay tuned for Episode 3, where tackle another common non-conformity: “<em>It is not clear what systematic search methods were used for the literature review.</em>” We’ll show you how to design and document a compliant search strategy that ensures transparency, traceability, and relevance in your literature review process. <a href="/newsletter">Sign up to our newsletter now so you don’t miss out</a>.</p>
  <hr />
  <div class="small-text">
    <strong>Transcript follows:</strong>
    <p>It’s Paul from Mantra Systems here, and welcome to episode two of our series on how to fix clinical evaluation non-conformances.</p>
    <p>If you didn’t see the first part in the series, I’ll put a link to that post in the description below this video.</p>
    <p>But otherwise, let’s get on with episode two.</p>
    <hr />
    <p>Non-conformances following submission of a clinical evaluation are quite common under the EU MDR, basically because the standard for submissions is now so high.</p>
    <p>But the problem is that:</p>
    <ul>
      <li>Resolution of non-conformances is necessary for the clinical evaluation to be accepted.</li>
      <li>Nonconformities can be quite difficult to interpret.</li>
      <li>It may not always be clear how to fully address them.</li>
      <li>A failure to resolve non-conformances may lead to:
        <ul>
          <li>A further round of review,</li>
          <li>Increased costs,</li>
          <li>Lost time,</li>
          <li>Stress and worry.</li>
        </ul>
      </li>
    </ul>
    <p>The aim of this series is to work through commonly seen nonconformities one at a time, and dig deep into what they mean and how to solve—or preferably avoid—them in the first place.</p>
    <p>The context behind this is to link into general principles for optimal conduct of medical device clinical evaluation.</p>
    <p><strong>Nonconformity Number Two.</strong> The one we’re going to look at is this:</p>
    <blockquote>
      <p><strong>‘Appraisal of literature articles has not been conducted appropriately.’</strong></p>
    </blockquote>
    <p>This is one we see a lot. To explore it—and ensure that you don’t fall foul of it—we’ve broken our approach down into three steps:</p>
    <ol>
      <li>Consider what appraisal actually means and what purpose it serves within the context of a clinical evaluation.</li>
      <li>Look at the characteristics of an appropriate appraisal framework.</li>
      <li>Consider how appraisal should be applied during clinical evaluation.</li>
    </ol>
    <p><strong>What Is Appraisal?</strong> Appraisal is:</p>
    <ul>
      <li>A <strong>structured and systematic evaluation</strong> of:
        <ul>
          <li>The <strong>quality</strong> of individual studies within a literature review or a clinical evaluation, and</li>
          <li>The <strong>relevance</strong> of each source to the clinical evaluation.</li>
        </ul>
      </li>
    </ul>
    <p>If applied correctly, appraisal enables <strong>systematic weighting</strong> of study outcomes according to quality and relevance.</p>
    <p>The basic idea:</p>
    <blockquote>
      <p>The better the study, and the more relevant it is, the greater weighting is given to its results compared with lower-quality studies of lower relevance.</p>
    </blockquote>
    <p>The MEDDEV guideline 2.7/1 revision 4—although somewhat outdated—places appraisal effectively into context.</p>
    <ul>
      <li>Clinical evaluation is split into four stages: 0, 1, 2, and 3.</li>
      <li><strong>Appraisal is stage 2</strong>, which:
        <ul>
          <li>Comes after identification of pertinent data,</li>
          <li>Leads into appropriate analysis,</li>
          <li>Emphasizes that you can’t properly analyze data until you’ve appraised it.</li>
        </ul>
      </li>
    </ul>
    <p><strong>Designing an Appraisal Framework</strong>. There is <strong>no universally defined appraisal system</strong>. Some systems are <strong>broadly accepted</strong>, such as the Oxford scheme, but:</p>
    <ul>
      <li>These can be limited. For example, the Oxford scheme only considers study type, overlooking other relevant factors.</li>
      <li>Therefore, <strong>justification of your chosen method is key</strong>.</li>
    </ul>
    <p>Appraisal should:</p>
    <ul>
      <li>Consider a <strong>range of factors</strong> related to study quality and relevance.</li>
      <li>Avoid being based on <strong>just one or two</strong> elements.</li>
    </ul>
    <blockquote>
      <p>The most common pitfall: <strong>doing nothing</strong> with the appraisal.</p>
    </blockquote>
    <p>The objective is to produce an <strong>appraisal score for each source</strong> that actively influences outcomes.</p>
    <p>This score becomes a <strong>mathematical weighting factor</strong> applied to results from individual sources.</p>
    <p><strong>Characteristics of a Compliant Appraisal Framework</strong>. There’s no universally accepted method, but an appraisal scheme may consider:</p>
    <ul>
      <li><strong>Study type</strong>
        <ul>
          <li>RCTs typically score higher than cohort studies or case series.</li>
        </ul>
      </li>
      <li><strong>Sample size</strong>
        <ul>
          <li>Larger sample sizes increase reliability and therefore the score.</li>
        </ul>
      </li>
      <li><strong>Use of statistics</strong>
        <ul>
          <li>Includes whether the study shows statistical significance.</li>
          <li>Well-designed statistical analysis increases score.</li>
        </ul>
      </li>
      <li><strong>Length of follow-up</strong>
        <ul>
          <li>Longer follow-up captures more of the clinical journey.</li>
        </ul>
      </li>
      <li><strong>Devices used within studies</strong>
        <ul>
          <li>Subject device: highest score</li>
          <li>Equivalent device: slightly lower</li>
          <li>Similar device: lower still</li>
          <li>Unrelated device: lowest score</li>
        </ul>
      </li>
    </ul>
    <p>The key is <strong>justification</strong>—explain to the reviewer why your approach is appropriate.</p>
    <p><strong>Example Scoring Framework.</strong> In your <strong>appraisal protocol</strong>, you might use a system like this:</p>
    <ul>
      <li>Characteristics listed on the left</li>
      <li>Descriptions and alpha-numeric grading on the right</li>
      <li>Higher numbers = better scores</li>
    </ul>
    <p>Example:</p>
    <ul>
      <li>A randomized control trial might be scored <code class="language-plaintext highlighter-rouge">R4</code>.</li>
      <li>A case series might be scored <code class="language-plaintext highlighter-rouge">R1</code>.</li>
    </ul>
    <p>You can build a table with these scores and apply it to each study. This allows a study to be:</p>
    <ul>
      <li>Strong in one area,</li>
      <li>Weak in another.</li>
    </ul>
    <p>This <strong>combined appraisal system</strong> accounts for nuance.</p>
    <p>To produce the <strong>overall appraisal score</strong>, add the numbers in each row.</p>
    <p>Example (hypothetical data):</p>
    <p>For ‘Jones et al’:<br />
      R3 (3) + 2 + 1 + 3 + 3 = <strong>12</strong></p>
    <p><strong>Applying the Appraisal.</strong> Once you have appraisal scores:</p>
    <ol>
      <li>List the studies and their appraisal scores.</li>
      <li>Add an outcome parameter (e.g., reduction in pain VAS).</li>
    </ol>
    <p>To calculate the <strong>weighted mean</strong>:</p>
    <ul>
      <li>Multiply the <strong>outcome value</strong> by the <strong>appraisal score</strong>.</li>
      <li>Example for Jones et al:
        <ul>
          <li>Outcome: 4.5</li>
          <li>Appraisal score: 12</li>
          <li>Weighted value: 4.5 × 12 = 54</li>
        </ul>
      </li>
    </ul>
    <p>Repeat for each source.</p>
    <p>Then:</p>
    <ul>
      <li>Sum the appraisal scores: <strong>61</strong></li>
      <li>Sum the weighted values: <strong>226.7</strong></li>
      <li>Divide the total weighted values by the total appraisal scores:</li>
    </ul>
    <pre><code class="language-math">Weighted mean = 226.7 / 61 = 3.72
</code></pre>
    <p>This gives <strong>greater weight to better quality studies</strong>, which is the essence of how appraisal is expressed in a compliant clinical evaluation.</p>
    <p><strong>Summary of an Effective appraisal:</strong></p>
    <ul>
      <li>Uses systematic methods to evaluate quality and relevance.</li>
      <li>Considers a range of factors.</li>
      <li>Produces a score to enable mathematical weighting of results.</li>
      <li>Must be:
        <ul>
          <li>Justified,</li>
          <li>Conducted according to a clear, advance plan.</li>
        </ul>
      </li>
      <li>Applies to literature for both:
        <ul>
          <li>Subject devices,</li>
          <li>State-of-the-art comparisons.</li>
        </ul>
      </li>
    </ul>
    <p><strong>Recap of Episode Two: Nonconformity #2: Appraisal of literature articles has not been conducted appropriately</strong></p>
    <p>In this episode, we’ve:</p>
    <ul>
      <li>Looked at what appraisal is and what purpose it serves.</li>
      <li>Considered characteristics of an appropriately conducted appraisal process.</li>
      <li>Looked at how appraisal should be applied during clinical evaluation and the effect it has on outcomes.</li>
    </ul>
    <hr />
    <p>Don’t forget that <strong>Mantra Systems offers a comprehensive clinical evaluation service</strong>. One of the most common reasons we’re engaged is to help with resolution of non-conformities.</p>
    <ul>
      <li>All initial conversations are <strong>completely free</strong>.</li>
      <li>We have a <strong>really friendly team</strong>.</li>
      <li>Give us a call or send us an email if you’d like to discuss anything.</li>
    </ul>
    <p>That concludes episode two in this non-conformity series.</p>
    <p><strong>Please share</strong> if you found this video useful, and I look forward to seeing you in the next part of the series.</p>
    <p><strong>Thank you very much!</strong></p>
  </div>
  ]]></content><author><name>paul-hercock</name></author><category term="Clinical Evaluation" /><category term="MDR" /><summary type="html"><![CDATA[Our ongoing series covers one of the most frequent reasons for CER rejection: a poor appraisal of literature sources.]]></summary></entry><entry><title type="html">Navigating CAPA Terminology: Key Terms for Medical Device Professionals</title><link href="https://www.mantrasystems.co.uk/articles/navigating-capa-terminology-key-terms-for-medical-device-professionals" rel="alternate" type="text/html" title="Navigating CAPA Terminology: Key Terms for Medical Device Professionals" /><published>2025-04-02T00:00:00+01:00</published><updated>2025-04-03T12:43:18+01:00</updated><id>https://www.mantrasystems.co.uk/articles/navigating-capa-terminology-key-terms-for-medical-device-professionals</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/navigating-capa-terminology-key-terms-for-medical-device-professionals"><![CDATA[<p>The Corrective and Preventive Action (CAPA) process is a central component of <a href="/eu-mdr-compliance/quality-management-system-qms">Quality Management Systems</a> (QMS) and is a mandatory requirement for ISO 13485, MDSAP, <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32017R0745#anx_%C2%A0VIII">Regulation MDR (EU) 2017/745</a>, and the <a href="/medical-device-regulatory-consulting-services/fda-510k-consulting">US FDA</a>. It is a systematic approach used to identify, address, and prevent the recurrence of problems.</p>
  <p>A CAPA is a structured process used to identify, resolve, and prevent the recurrence of issues within a system or product. It is essential for ensuring quality, compliance, and continuous improvement in various industries such as healthcare, manufacturing, and pharmaceuticals in the UK. The CAPA process is typically broken down into three main phases: <strong>Identify Problems</strong>, where issues are first recognised and thoroughly assessed; <strong>Address Problems</strong>, where corrective actions are formulated and implemented to fix the immediate problems; and <strong>Prevent Recurrence</strong>, where long-term solutions and preventive measures are established to ensure that the issue does not happen again, ultimately enhancing the reliability and quality of operations.</p>
  <figure>
    <img src="/assets/images/content/articles/inline/capa-process-in-quality-management.svg" alt="CAPA Process in Quality Management diagram." loading="lazy" width="520" height="416" />
    <figcaption>CAPA Process in Quality Management</figcaption>
  </figure>
  <h2 id="key-terms">Key terms</h2>
  <p>Here are some key terms that all medical device professionals should acquaint themselves with:</p>
  <h3 id="nonconformity--a-deviation-from-a-specification-standard-or-an-expectation">Nonconformity = A deviation from a specification, standard, or an expectation</h3>
  <p>Nonconformity is the recognition of an issue that deviates from the expected outcome or requirement. These typically appear at the beginning of the process, during identification and investigation stages.</p>
  <h3 id="capa-trigger--a-predefined-condition-that-leads-to-a-corrective-or-preventative-measure">CAPA Trigger = A predefined condition that leads to a corrective or preventative measure</h3>
  <p>CAPA triggers can arise from various sources, both internal and external, and typically highlight areas where a product, process, or system does not meet specified standards or expectations. Examples of CAPA triggers include: customer complaints, audit findings, and adverse events or near misses.</p>
  <h3 id="root-cause-analysis-rca--a-method-to-identify-the-cause-of-a-problem">Root Cause Analysis (RCA) = A method to identify the cause of a problem</h3>
  <p>The goal of RCA is not just to address the immediate symptoms of a problem, but to discover and resolve the root cause(s) to prevent recurrence. It’s a critical component of the CAPA process, especially in regulated industries like manufacturing, healthcare, and quality control. Tools such as the 5 Whys, Fishbone Diagram, and Failure Mode and Effects Analysis (FMEA) can be used to facilitate RCA.</p>
  <h3 id="root-cause--the-main-reason-why-a-problem-has-arisenoccurred">Root Cause = The main reason why a problem has arisen/occurred</h3>
  <p>In any quality management or problem-solving process, identifying the root cause is critical because it allows an organisation to focus its efforts on eliminating the source of the problem, rather than just addressing its symptoms.</p>
  <h3 id="field-corrective-action-fca--actions-to-address-issues-identified-after-distribution">Field Corrective Action (FCA) = Actions to address issues identified after distribution</h3>
  <p>FCAs are typically initiated after a product defect, safety concern, or performance failure has been identified in the field, often through customer complaints, product recalls, or post-market surveillance data. They form an important part of the product lifecycle and help in protecting both the end users and the manufacturer’s reputation.</p>
  <h3 id="supplier-corrective-action-request-scar--a-request-to-a-supplier-to-address-the-problem">Supplier Corrective Action Request (SCAR) = A request to a supplier to address the problem</h3>
  <p>The SCAR process is used to communicate nonconformities, initiate corrective actions, and ensure that suppliers address the underlying issues causing defects or quality problems. It’s an essential tool in quality management systems. The purpose of SCAR is to improve supplier performance and prevent recurrence of issues that may affect the quality and safety of the end product.</p>
  <h3 id="capa-plan--a-documented-plan-for-actions-to-address-a-specific-issue">CAPA Plan = A documented plan for actions to address a specific issue</h3>
  <p>It begins with identifying a problem, conducting a root cause analysis, and implementing corrective actions to resolve the issue. Preventive actions are then put in place to reduce the risk of future occurrences. The plan also includes verifying the effectiveness of these actions, documenting the process, and ensuring continuous monitoring for improvement.</p>
  <h3 id="capa-review-board--a-group-that-is-responsible-for-overseeing-the-capa-process">CAPA Review Board = A group that is responsible for overseeing the CAPA process</h3>
  <p>The board ensures that identified issues are appropriately investigated, root causes are thoroughly analysed, and corrective and preventive actions are effectively implemented.</p>
  <h3 id="risk-assessment--the-process-of-identifying-and-analysing-the-impact-of-potential-issues">Risk Assessment = The process of identifying and analysing the impact of potential issues</h3>
  <p>In the context of CAPA, risk assessment helps determine the severity and likelihood of an issue reoccurring, allowing companies to implement appropriate actions to mitigate or control these risks. It plays a critical role in ensuring that corrective and preventive measures are effective and proportionate to the identified risks.</p>
  <h3 id="correction--action-to-fix-a-nonconformity-without-addressing-the-root-issue">Correction = Action to fix a nonconformity without addressing the root issue</h3>
  <p>Unlike corrective actions, which are aimed at preventing recurrence, corrections focus on resolving the current issue.</p>
  <h3 id="preventative-action-pa--actions-that-address-the-causes-of-a-potential-nonconformity">Preventative Action (PA) = Actions that address the causes of a potential nonconformity</h3>
  <p>This process involves analysing trends, risks, or weak points in the system and making improvements to avoid future problems, ensuring ongoing compliance and quality.</p>
  <h3 id="corrective-action-ca--actions-that-address-the-causes-of-an-existing-nonconformity">Corrective Action (CA) = Actions that address the causes of an existing nonconformity</h3>
  <p>This process involves investigating the issue, implementing changes to eliminate the cause, and ensuring that similar issues do not arise in the future.</p>
  <h3 id="verification--checking-that-a-solution-works-as-intended">Verification = Checking that a solution works as intended</h3>
  <p>Verification is the process of confirming that a product, process, or system meets specific requirements and functions as intended. It involves checking and testing against predetermined criteria to ensure accuracy and compliance before implementation or release.</p>
  <h3 id="validation--ensuring-that-a-product-or-a-process-meets-its-intended-use">Validation = Ensuring that a product or a process meets its intended use</h3>
  <p>Validation ensures that a product, process, or system consistently meets its intended purpose and regulatory requirements. It involves testing and documentation to confirm reliability and compliance with standards, ensuring product quality and safety.</p>
  <h3 id="effectiveness-check-voe--evaluation-to-confirm-the-success-of-a-capa">Effectiveness Check (VoE) = Evaluation to confirm the success of a CAPA</h3>
  <p>After implementing corrective actions, an Effectiveness Check ensures that the changes have successfully resolved the problem and that the same issue does not reoccur. This check typically involves monitoring the results over a set period, reviewing data, and evaluating the impact of the actions on product quality, performance, or process improvements.</p>
  <h3 id="disposition--decision-on-how-to-handle-a-nonconforming-product-or-situation">Disposition = Decision on how to handle a nonconforming product or situation</h3>
  <p>Disposition refers to the decision made regarding nonconforming products, determining whether they should be reworked, used as-is, scrapped, or returned to the supplier. These decisions are based on the severity of the nonconformity and are crucial for maintaining product quality and compliance.</p>
  <h2 id="mastering-capa-turning-terminology-into-action">Mastering CAPA: Turning Terminology into Action</h2>
  <p>In conclusion, understanding and navigating CAPA terminology is essential for ensuring compliance and driving continuous improvement within a quality management system. Familiarity with key terms such as root cause analysis, corrective actions, preventive actions, and effectiveness checks will help professionals make informed decisions, streamline processes, and avoid common pitfalls.</p>
  <p>By staying up-to-date with these critical terms, organisations can strengthen their CAPA processes, improve product quality, and enhance overall regulatory compliance. Empowering teams with the knowledge of CAPA terminology is a key step toward maintaining high-quality standards and ensuring the safety and efficacy of medical devices in the marketplace.</p>
  <p>Ready to navigate the regulatory process with confidence? With our expert team and 100% notified body approval, we’re here to help you ensure compliance and streamline your medical device journey. <a href="/contact">Contact us today to get started!</a></p>
  ]]></content><author><name>kamiya-crabtree</name></author><category term="MDR" /><summary type="html"><![CDATA[We define and explain the language required to work within a Quality Management System (QMS).]]></summary></entry><entry><title type="html">Clinical Evaluation Masterclass: Overcoming Non-conformities - Episode 1</title><link href="https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-1-overcoming-non-conformities" rel="alternate" type="text/html" title="Clinical Evaluation Masterclass: Overcoming Non-conformities - Episode 1" /><published>2025-04-02T00:00:00+01:00</published><updated>2025-04-09T12:26:25+01:00</updated><id>https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-1-overcoming-non-conformities</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/clinical-evaluation-masterclass-1-overcoming-non-conformities"><![CDATA[<p>Clinical Evaluation Plans (CEPs) and Clinical Evaluation Reports (CERs) are critical components of medical device regulatory compliance. However, the reality is that meeting requirements isn’t always plain sailing.</p>
  <p>Based on our experience with over 250 successful submissions, we’ve identified that overcoming clinical evaluation non-conformities is one of the major challenges manufacturers face during the device approval journey.</p>
  <h2 id="what-is-a-non-conformity-in-clinical-evaluation">What is a Non-Conformity in Clinical Evaluation?</h2>
  <p>A non-conformity arises when a CEP or CER fails to meet MDR requirements. Non-conformities signal that the evaluation does not yet adequately demonstrate a medical device’s safety and performance or that full alignment with regulatory obligations has not been shown, requiring correction before approval.</p>
  <h2 id="the-impact-of-non-conformities-on-manufacturers">The Impact of Non-Conformities on Manufacturers</h2>
  <p>Non-conformities significantly disrupt the approval process, leading to extended review timelines, increased costs, and mandatory revision cycles. Since notified bodies (NBs) cannot accept a CEP or CER with unresolved non-conformities, addressing them is critical for market access.</p>
  <h2 id="a-helping-hand">A helping hand</h2>
  <p>Our new video series applies our deep experience to help manufacturers proactively identify and fix these issues for smoother audits and faster certifications. This series breaks down the most common clinical evaluation non-conformities, explains <em>why</em> they keep happening, and most importantly, shows you <em>how to fix them</em> for good. Each episode provides step-by-step guidance to help you avoid these pitfalls in your CER submissions.</p>
  <h2 id="episode-1-safety-and-performance-objectives-lack-specific-and-measurable-acceptance-criteria">Episode 1: “Safety and Performance Objectives Lack Specific and Measurable Acceptance Criteria”</h2>
  <p>This frequently cited regulatory observation typically reflects not an actual safety or performance deficiency, but rather insufficiently defined evaluation parameters. The episode will address:</p>
  <ul>
    <li>Fundamental concepts of safety and performance objectives</li>
    <li>Deriving safety and performance objectives using weighted values</li>
    <li>Defining “specific” and “measurable” objectives</li>
    <li>Practical approaches to establishing and implementing acceptance criteria</li>
  </ul>
  <p>Coming in Episode 2: We’ll tackle another frequent non-conformity, “<em>Appraisal of literature articles has not been conducted appropriately</em>” showing you how to strengthen your literature review process. <a href="/newsletter">Sign up to our newsletter now so you don’t miss out</a>.</p>
  <hr />
  <div class="small-text">
    <strong>Transcript follows:</strong>
    <p>Okay, hi everyone, it’s Paul here from Mantra Systems. Welcome to a brand new series that we’re running on how to fix <strong>clinical evaluation non-conformities</strong>.</p>
    <p>The idea behind this series is to equip you with the capability to, first of all, avoid common non-conformities that are seen in medical device clinical evaluations. And then secondly, if you have been unfortunate enough to receive one, by the end of the series you should hopefully have a powerful strategy for correcting them and moving on to full acceptance of your CE.</p>
    <p>So, just for this first episode, we’re going to cover a few basics, and it’s worth starting with the question: <em>What is a clinical evaluation non-conformity?</em></p>
    <p>Well, obviously, apart from Class I devices, clinical evaluation plans and clinical evaluation reports need to be submitted for review by a notified body. And a non-conformity — or NC for short — is when conformity with requirements has not been adequately demonstrated. A non-conformity requires correction and then resubmission of the CEP and CER. Resolution — certainly of major non-conformities — is necessary in order for the clinical evaluation to be accepted.</p>
    <p>So, it is a really important topic, and non-conformities under MDR are quite common. But the problem with non-conformities is that sometimes the questions — which is how they’re often manifest, it’s a question from a reviewer — they can be difficult to interpret. Sometimes it’s not clear exactly what a reviewer means, and it may be unclear how to fully address them as well.</p>
    <p>A failure to resolve non-conformities in full may lead to a further round of review following yet another resubmission, which is just multiplying costs, losing time, and it leads to stress and worry — because it is anxiety-inducing trying to get a clinical evaluation through approval.</p>
    <p>Okay, so the aim of the series is to work through common non-conformities one at a time, and to dig deep into what they mean and how to solve or avoid them. The series and the principles within it link over to general principles for optimal conduct of clinical evaluation under the MDR.</p>
    <p>So, let’s start with our first example non-conformity. This one is a really common one, and it states:</p>
    <blockquote>
      <p><em>“Safety and performance objectives do not appear to have specific and measurable acceptance criteria.”</em></p>
    </blockquote>
    <p>Breaking that down, we need to understand exactly what the question means and how to address it. It’s possible to break it down into the following bullet points:</p>
    <ul>
      <li>What actually are safety and performance objectives?</li>
      <li>What in this context does “specific and measurable” mean?</li>
      <li>What are acceptance criteria?</li>
    </ul>
    <p>And importantly, of course, it needs to go beyond just knowing what they are but: <strong>How do we derive and analyse them?</strong></p>
    <p>So, let’s begin with safety and performance objectives. What are safety and performance objectives?</p>
    <p>Well, these ultimately are benchmark values against which the device under evaluation will be compared. They are derived from the state-of-the-art literature review, which is an essential component of clinical evaluation.</p>
    <p>Technically speaking, objectives have two components. They’re derived of a clinical outcome parameter — which is a qualitative concept or type of outcome — and then attached to that, there will be a quantitative value that forms the actual objective.</p>
    <p>As per the non-conformity, safety and performance objectives must be specific and measurable. Now, that covers a lot of ground, so let’s take an example and break this down.</p>
    <p>An example might be of a completed safety — or in this case a performance — objective:</p>
    <blockquote>
      <p>“Increasing walking distance in meters at six weeks post-procedure of 61.46m.”</p>
    </blockquote>
    <p>You can see within this the two components: the “increase in walking distance in meters at six weeks” — that’s not a value, okay? It contains a number, but that’s just because we need to compare like with like. That’s not the actual objective.</p>
    <p>The clinical outcome parameter is a thing, a concept, something you might measure, something to which you might attach a value. And then the second part of it is the actual performance outcome, which is quantitative.</p>
    <p>These two things together constitute a specific and measurable safety or performance objective.</p>
    <p>So, how are these derived? Well, they’re derived during the analysis stage of a state-of-the-art literature review. We’re going to go on to a working example in a moment to show exactly how they are derived.</p>
    <p>Remember, a clinical outcome parameter is a “thing,” and you decide upon those by looking for outcomes that are seen in a comparable form across multiple sources within the state-of-the-art literature review. If you have four, five, six sources all reporting the same type of outcome, it’s a strong candidate for a clinical outcome parameter.</p>
    <p>In the state-of-the-art protocol, there should be a method for determining what constitutes a clinical outcome parameter. That, of course, would feed into the clinical evaluation plan.</p>
    <p>Safety and performance objectives then are weighted mean values attached to those parameters. As we’ve seen before, you need both parts.</p>
    <p>It’s worth at this point just reflecting on the principles of an effective state-of-the-art literature review. Because there’s no point producing state-of-the-art safety or performance outcomes that have not been derived properly.</p>
    <p>We’ll go into this in another video, but a state-of-the-art literature review requires:</p>
    <ul>
      <li>a detailed protocol</li>
      <li>use of a validated method to define research questions and search terms (one such method is PICCO)</li>
      <li>a well-documented literature extraction process with recording and justification of all excluded sources,</li>
      <li>a structured appraisal — which is where we get to be very specific to the production of safety and performance objectives, and</li>
      <li>the analysis — which is where the actual objectives are produced.</li>
    </ul>
    <p>But we cover that in a lot more detail in a separate video.</p>
    <p>So, let’s look at a working example of how safety and performance objectives are derived.</p>
    <p>Here is fictional data. Let’s say we’ve done a state-of-the-art literature review, and sticking with the same example, we found four publications that all reported a mean increase in walking distance at six weeks, and they all did it in a comparable way. Here are the mean values from each of those sources.</p>
    <p>How do we produce a weighted mean?</p>
    <p>You can see in the table that the way we’re going to weight it is by a representation of the quality of each of these studies, to ensure that greater weighting or prominence is given to results from higher quality studies. In order to do that, we need to produce an appraisal score.</p>
    <p>Appraisal scores — there are lots of different ways to do this. On the next slide we’ve got, again, a simplified fictional example of how to calculate an appraisal score. You might look at different aspects of the study: you might consider study type, sample size, the use of statistical tests and whether they were appropriate, and the length of follow-up. You’d probably in reality consider other factors as well, but for the purpose of this, this will suffice.</p>
    <p>You’ve perhaps seen this — usually these manifest as an alphanumeric code, and again, the meaning behind this code should be expressed in the literature search protocol, in the appraisal plan section. So this will all be mapped out.</p>
    <p>Ultimately, this enables the calculation of a numerical appraisal score for each paper, with a higher number representing a higher quality study. Then those values can be imputed back into the original table with all the other values being the same.</p>
    <p>Now we have an appraisal score by which we can weight the results from each study. That’s done fairly simply just by multiplying the actual result by the appraisal score. We do that for all of them to produce a weighted value for each study.</p>
    <p>But we’re not quite done there, because we need to calculate a weighted mean. That’s done by taking the sum of all the weighted values and dividing that by the sum of the appraisal scores.</p>
    <p>What we’re doing there is making the appraisal scores the denominator — they are the weighting factor that’s being applied across all studies. This means we achieve our objective of moving the weighted mean closest to the values of the highest quality studies.</p>
    <p>In this case, that means: 3872 / 63, which gives a weighted mean of 61.46.</p>
    <p>It’s always worth sanity checking these, but if you look at the plain mean values from each study and the appraisal scores, you’d expect it to land around the 60 mark — and that’s where it falls: 61.46.</p>
    <p>So, that’s a simplified example of how to calculate a weighted mean for the quantitative component of a safety and performance objective.</p>
    <p>Don’t forget, the non-conformity required that we produce safety and performance objectives that are specific and measurable.</p>
    <p>Let’s just reflect on what that means:</p>
    <ul>
      <li>Specific means unambiguous, clear, and would be consistently interpreted as meaning the same thing.</li>
      <li>Measurable means it contains a value — a quantitative value — against which another value from another device can be measured.</li>
    </ul>
    <p>So we have this in this case. It’s clear that that is both specific and measurable, and that’s why we included reference to six weeks — to ensure we’re comparing like with like.</p>
    <p>That leaves us to consider the final aspect of the non-conformity, which was reference to acceptance criteria.</p>
    <p>An acceptance criteria basically defines when safety or performance of the device under evaluation are acceptable in comparison to state-of-the-art.</p>
    <p>What we’re doing here is setting out a means for comparing the device-under-evaluation outcomes with the state-of-the-art objectives for specified clinical outcome parameters. By now, all of these terms should have a meaning attached to them.</p>
    <p>So, the clinical evaluation plan needs to contain an analysis plan for how this comparison will be done. Suffice to say that the outcomes for the device under evaluation are calculated by weighted means using a very similar method that we used for the state-of-the-art objectives.</p>
    <p>And acceptance — this is the key part — acceptance can be defined as showing (statistically) that the outcomes for the device under evaluation are non-inferior to objectives derived from the SOA. We don’t need to show superiority — we’re just showing non-inferiority. That’s a key distinction.</p>
    <p>Let’s take a final example, then, working this through.</p>
    <p>We can see:</p>
    <ul>
      <li>Increasing walking distance (spelled correctly this time) of 61.46,</li>
      <li>and in this example there were some other clinical outcome parameters as well:
        <ul>
          <li>improvement in pain (VAS, a measure of pain score),</li>
          <li>and range of motion at six months.</li>
        </ul>
      </li>
    </ul>
    <p>We have values attached to all of those. Then we also have weighted mean values for the device under evaluation.</p>
    <p>Remember, the job here is to determine whether the device under evaluation is non-inferior to SOA objectives.</p>
    <p>For the top one, it’s very easy, because a plain number comparison shows that the device under evaluation did better than the SOA objective. We don’t need to do any fancy tests — it’s obviously non-inferior. That’s a straightforward conclusion.</p>
    <p>But with the others, on a plain number comparison, the device under evaluation actually looks like it’s done less well than the state-of-the-art objectives. The important thing to understand here is whether that represents true inferiority, or whether these values are statistically non-different.</p>
    <p>For that reason, conducting a statistical test is relevant. Often a t-test is an appropriate test because it’s a comparison of means, and it generates a p-value denoting significance or non-significance of the difference between these values.</p>
    <p>Let’s say we conducted a t-test in this case, and these were the p-values that were derived. Generally, significance requires a value of less than 0.05 for the p-value. That’s not what we’re seeing here, and so these differences were non-inferior.</p>
    <p>According to our acceptance criteria, the device under evaluation has been shown to have appropriate performance in reference to state-of-the-art.</p>
    <p>So let’s go back to the original non-conformity:</p>
    <blockquote>
      <p>“Safety and performance objectives do not appear to have specific and measurable acceptance criteria.”</p>
    </blockquote>
    <p>We covered:</p>
    <ul>
      <li>what safety and performance objectives are,</li>
      <li>how to derive them using weighted values following a state-of-the-art literature review,</li>
      <li>what “specific and measurable” means and how the outlined process generates specific and measurable objectives,</li>
      <li>and what acceptance criteria are and how to apply them — including through use of statistical testing.</li>
    </ul>
    <p>If you need any further support in relation to clinical evaluation or working through non-conformities, Mantra Systems are clinical evaluation specialists. You’re free to contact our team at any time.</p>
    <p>If you’ve just got some general questions, don’t worry — it’s absolutely fine. More than happy to speak at any time. So feel free to reach out to us if you need any additional support.</p>
    <p>That concludes the first episode of the Clinical Evaluation Non-Conformity series. I’d like to thank you very much, and if you have any questions or comments, please let me know below the video.</p>
    <p>Thank you very much.</p>
  </div>
  ]]></content><author><name>paul-hercock</name></author><category term="Clinical Evaluation" /><category term="MDR" /><summary type="html"><![CDATA[In this series, we work step-by-step through common Non-Conformities to ensure you are always ahead of possible challenges on the way to MDR approval.]]></summary></entry><entry><title type="html">MDCG 2020-16 Revision 4: Key Updates on IVDR Classification Rules</title><link href="https://www.mantrasystems.co.uk/articles/mdcg-2020-16-rev-4-key-updates-on-ivdr-classification-rules" rel="alternate" type="text/html" title="MDCG 2020-16 Revision 4: Key Updates on IVDR Classification Rules" /><published>2025-03-28T00:00:00+00:00</published><updated>2025-03-28T10:50:34+00:00</updated><id>https://www.mantrasystems.co.uk/articles/mdcg-2020-16-rev-4-key-updates-on-ivdr-classification-rules</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/mdcg-2020-16-rev-4-key-updates-on-ivdr-classification-rules"><![CDATA[<p>The <strong>Medical Device Coordination Group (MDCG)</strong> has released <strong>MDCG 2020-16 rev.4 - Guidance on Classification Rules for in vitro Diagnostic Medical Devices under Regulation (EU) 2017/746</strong>.</p>
  <p>This update released in March 2025 provides comprehensive clarification on the classification rules outlined in <strong>Annex VIII</strong> of the <strong>IVDR</strong>, ensuring that manufacturers, notified bodies, and health institutions have a clear framework for classifying in vitro diagnostic (IVD) devices based on their intended purpose and inherent risk, prior to placing it on the market. Notably, the guidance addresses the classification of medical device software (MDSW) and emphasises the importance of considering devices used in combination.</p>
  <h2 id="summary-of-key-updates">Summary of Key Updates</h2>
  <table>
    <thead>
      <tr>
        <th style="text-align: left">Rule</th>
        <th style="text-align: left">Changes</th>
        <th style="text-align: left">Details</th>
        <th style="text-align: left">Impact</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td style="text-align: left">Rule 1 – Second Indent</td>
        <td style="text-align: left">Revision of examples</td>
        <td style="text-align: left">Updated examples to provide better clarity on the classification of IVD devices.</td>
        <td style="text-align: left">Ensures more precise classification, reducing ambiguity for manufacturers.</td>
      </tr>
      <tr>
        <td style="text-align: left">Rule 3(m)</td>
        <td style="text-align: left">Footnote 8 modified</td>
        <td style="text-align: left">Footnote 8 modified to refine interpretation and applicability.</td>
        <td style="text-align: left">Improves understanding of specific classification aspects.</td>
      </tr>
      <tr>
        <td style="text-align: left">Rule 4(a)</td>
        <td style="text-align: left">Revision of examples</td>
        <td style="text-align: left">Updated examples for self-testing devices, refining classifications based on biomarkers.</td>
        <td style="text-align: left">Helps manufacturers align with IVDR requirements for self-testing IVDs.</td>
      </tr>
      <tr>
        <td style="text-align: left">Rule 6</td>
        <td style="text-align: left">Minor modification of the rationale and revision of examples</td>
        <td style="text-align: left">Minor updates to rationale and refined examples for Class B IVDs.</td>
        <td style="text-align: left">Provides a clearer framework for determining Class B device classification.</td>
      </tr>
      <tr>
        <td style="text-align: left">Annex 1 and Annex 2</td>
        <td style="text-align: left">Minor editorial changes</td>
        <td style="text-align: left">Minor wording and structural refinements for improved readability.</td>
        <td style="text-align: left">Enhances document clarity without changing regulatory requirements.</td>
      </tr>
    </tbody>
  </table>
  <h2 id="key-considerations-for-manufacturers">Key Considerations for Manufacturers</h2>
  <p>The <a href="https://health.ec.europa.eu/document/download/12f9756a-1e0d-4aed-9783-d948553f1705_en?filename=md_mdcg_2020_guidance_classification_ivd-md_en.pdf">MDCG 2020-16 rev.4 guidance</a> has significant implications for <strong>IVD manufacturers</strong> as it refines how devices should be classified under IVDR.</p>
  <ul>
    <li><strong>Accurate Classification Criteria</strong> – The revision of examples under <strong>Rules 1, 4(a), and 6</strong> helps manufacturers correctly classify their IVD devices, reducing uncertainty. Manufacturers must <strong>review and adapt</strong> classifications, ensuring conformity with <strong>IVDR and notified body requirements</strong>.</li>
    <li><strong>More Precise Risk Assessment</strong> – Manufacturers need to reassess whether their devices fall into a different risk class, impacting regulatory requirements.</li>
    <li><strong>Improved Compliance with IVDR</strong> – The minor changes provide clarity but do not fundamentally alter the classification process. However, manufacturers should review their classification decisions against the revised examples.</li>
    <li><strong>Potential Impact on Notified Body Review</strong> – If an IVD device’s classification is affected by the updated examples or rationale, additional <strong>notified body consultation</strong> may be required for conformity assessments.</li>
  </ul>
  <h2 id="final-thoughts">Final Thoughts</h2>
  <p>The MDCG has provided essential updates to support <strong>IVD manufacturers</strong> in <strong>correctly interpreting classification rules under the IVDR</strong>. By understanding and implementing these refinements, manufacturers can <strong>ensure accurate device classification, strengthen regulatory compliance, and streamline interactions with notified bodies</strong>.</p>
  <p>For more insights on IVDR compliance and regulatory transitions, you may find these articles helpful:</p>
  <ul>
    <li><a href="/articles/understanding-risk-based-classification-of-in-vitro-medical-devices-under-eu-ivdr">Understanding Risk-Based Classifications for IVDs Under the IVDR</a></li>
    <li><a href="/articles/ivdd-ivdr-ensuring-compliance-during-transitional-period">Transitioning from IVDD to IVDR: Key Considerations</a></li>
  </ul>
  <p>Navigating IVDR classification can be challenging. <strong>Mantra Systems</strong> provides <strong>expert regulatory consulting, medical writing, and compliance support</strong> to help manufacturers <strong>interpret and implement</strong> the latest MDCG guidance seamlessly. <a href="/contact">Contact us today</a> for a free, confidential consultation on how we can support your IVDR compliance journey.</p>
  ]]></content><author><name>chandini-vk</name></author><category term="IVDR" /><summary type="html"><![CDATA[We detail how this revision ensures manufacturers, notified bodies, and health institutions have a clearer regulatory framework.]]></summary></entry><entry><title type="html">Top 5 Common Pitfalls to Avoid During Risk Assessment</title><link href="https://www.mantrasystems.co.uk/articles/top-5-risk-assessment-pitfalls-how-to-avoid-them" rel="alternate" type="text/html" title="Top 5 Common Pitfalls to Avoid During Risk Assessment" /><published>2025-03-26T00:00:00+00:00</published><updated>2025-03-26T10:48:01+00:00</updated><id>https://www.mantrasystems.co.uk/articles/top-5-risk-assessment-pitfalls-how-to-avoid-them</id><content type="html" xml:base="https://www.mantrasystems.co.uk/articles/top-5-risk-assessment-pitfalls-how-to-avoid-them"><![CDATA[<p>A medical device-specific risk assessment uses the <a href="https://www.iso.org/standard/72704.html">ISO 14971 standard</a>, which provides guidance for producing and maintaining a risk management file.</p>
  <p>A risk assessment is a process used to identify, analyse, and mitigate risk in the design and development process to enable the sale of safe and successful medical devices. <a href="/eu-mdr-compliance/risk-management">Risk assessment</a> ensures that all potential hazards are identified and mitigated to protect patient safety. However, manufacturers often encounter mistakes along the way that can compromise the effectiveness of the assessment and delay product launch. To help you navigate the risk management process smoothly, here are the <strong>Top 5 Pitfalls</strong> to avoid during the risk assessment of your medical devices.</p>
  <figure>
    <img src="/assets/images/content/articles/inline/5-pitfalls-for-risk-assessment.svg" alt="Diagram highlighting the 5 pitfalls for risk assessment." loading="lazy" width="498" height="552" />
  </figure>
  <h2 id="1-failing-to-identify-all-potential-hazards">1. Failing to Identify All Potential Hazards</h2>
  <p>To begin a risk assessment, a comprehensive identification of potential hazards that could arise at any stage of the device’s lifecycle should be conducted. This involves documenting known and foreseeable hazards related to the medical device, considering its intended use, foreseeable misuse, and safety characteristics. A significant pitfall occurs here when manufacturers underestimate these hazards, in turn leading to an incomplete risk assessment.</p>
  <p><strong>How to avoid this:</strong></p>
  <p>Ensure cross-disciplinary teams are engaged (e.g., engineering, clinical, regulatory experts). This will ensure a thorough hazard analysis. Consider all potential risks, including those related to device malfunction, misuse, environmental factors, and human error.</p>
  <h2 id="2-insufficient-documentation-and-traceability">2. Insufficient Documentation and Traceability</h2>
  <p><a href="/enable-ce-mark/online-video-training-courses/risk-management">Clear documentation</a> of the risk management process is required for regulatory authorities, e.g., Notified Bodies, to review. <a href="https://eur-lex.europa.eu/legal-content/EN/TXT/HTML/?uri=CELEX:32017R0745">Article 10 of the EU MDR</a> requires all manufacturers to establish, document, implement, and maintain a system for Risk Management. A frequent pitfall here is failing to maintain proper documentation of hazard identification, risk analysis, and control measures. This, in turn, will lead to non-compliance and delays.</p>
  <p><strong>How to avoid this:</strong></p>
  <p>Confirm that every step of the risk management process is documented in detail. Alongside this, you can use risk management tools, e.g., FMEA or risk matrices, to track the rationale behind each decision. This will allow for full traceability throughout the device lifecycle.</p>
  <p>Maintaining and updating a Risk Management strategy requires scheduled review and appraisal sessions to analyse system suitability. The clinical evaluation cycle offers an opportunity to assimilate Risk Management data collected and to re-perform a benefit-risk analysis of the device. Any updates or changes to the process must be reflected in documentation and disseminated across the organisation to ensure the changes are implemented.</p>
  <h2 id="3-underestimating-risk-acceptability-criteria">3. Underestimating Risk Acceptability Criteria</h2>
  <p>Determining whether a risk is acceptable or not is crucial in risk assessment. During risk evaluation, the manufacturer must assess estimated risks for each hazardous situation and check if they meet the criteria in the risk management plan.</p>
  <p>Setting inappropriate risk acceptance thresholds can directly impact both patient safety and product timelines. If the acceptable risk is set too high, the device may pose a significant threat to users, leading to potential harm or regulatory non-compliance. On the other hand, being too cautious and lowering the threshold could result in unnecessary design changes or delays in the approval process, even for risks that are statistically insignificant.</p>
  <p><strong>How to avoid this:</strong></p>
  <p>Risk acceptance should be based on both regulatory requirements and real-world clinical needs. The criteria should be proportional to the severity and likelihood of the identified risks. A low-risk device may have higher acceptable risk levels, while a high-risk device, such as a life-supporting medical device, must have stringent limits. Tools like risk matrices can help categorise and prioritise risks by their severity and probability, ensuring a rational and evidence-based approach to setting acceptability levels.</p>
  <h2 id="4-ignoring-post-market-risks">4. Ignoring Post-Market Risks</h2>
  <p>Many manufacturers place a significant emphasis on pre-market risks, neglecting the post-market risks that may arise once the device is in use by patients or healthcare professionals. The manufacturer must review all collected post-market information, focusing on safety relevance. This aims to identify unrecognised hazards not initially identified, hazardous situations with now-unacceptable estimated risks, and cases where the overall residual risk is no longer acceptable relative to intended use benefits.</p>
  <p><strong>How to avoid:</strong></p>
  <p>Ensure you incorporate a robust <a href="/eu-mdr-compliance/post-market-surveillance-pms">post-market surveillance plan</a> into your risk management strategy. As well as this, make sure you set up systems to monitor and track device performance, adverse events, and potential hazards once the product is on the market.</p>
  <h2 id="5-neglecting-risk-control-and-mitigation-strategies">5. Neglecting Risk Control and Mitigation Strategies</h2>
  <p>The identification of hazards is just one part of the risk management process. The next critical phase is the implementation of effective risk controls, which is necessary to reduce identified risks to acceptable levels. Failing to implement effective controls or using controls that are impractical or inadequate is a common trap. This can lead to unresolved risks that may jeopardise patient safety, compromise device effectiveness, and increase the likelihood of regulatory non-compliance.</p>
  <p><strong>How to avoid:</strong></p>
  <p>To avoid neglecting risk control strategies, it’s essential to follow a structured approach that aligns with recognised best practices and regulatory standards. Here are some practical steps:</p>
  <ul>
    <li>Start by prioritising the risks that need mitigation based on their severity and probability of occurrence.</li>
    <li>Evaluate the effectiveness of controls. This can include pre-market validation through clinical trials, in-vitro testing, and failure testing, as well as post-market monitoring to ensure that controls remain effective over time.</li>
    <li>For each identified risk, provide a rationale for the selected control, how it works to mitigate the hazard, and how its effectiveness will be measured.</li>
  </ul>
  <h2 id="final-thoughts-mastering-risk-assessment">Final Thoughts: Mastering Risk Assessment</h2>
  <p>By carefully identifying all potential hazards, setting realistic risk acceptability criteria, and implementing robust mitigation strategies, you can significantly reduce the likelihood of regulatory setbacks and safety issues. A proactive and comprehensive approach to risk management not only protects patients but also streamlines the path to market, helping your device succeed in a competitive landscape. Remember, risk assessment is an ongoing process that requires constant vigilance and adaptation, so stay committed to continuous improvement. Please <a href="/contact">contact us if you need any help with risk assessment</a> or any medical device regulation issue.</p>
  ]]></content><author><name>kamiya-crabtree</name></author><category term="MDR" /><summary type="html"><![CDATA[Learn how to sidestep costly mistakes which manufacturers commonly make. From hazard ID to post-market surveillance, we help you improve safety and speed up approvals.]]></summary></entry></feed>